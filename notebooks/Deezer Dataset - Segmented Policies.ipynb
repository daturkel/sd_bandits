{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from obp.ope import ReplayMethod\n",
    "from obp.policy import EpsilonGreedy, BernoulliTS\n",
    "from sd_bandits.obp_extensions.policy import ExploreThenCommit, SegmentPolicy \n",
    "from obp.simulator import run_bandit_simulation\n",
    "from obp.utils import convert_to_action_dist\n",
    "\n",
    "from sd_bandits.obp_extensions.dataset import DeezerDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Deezer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_path = \"../data/deezer_carousel_bandits/user_features.csv\"\n",
    "playlist_features_path = \"../data/deezer_carousel_bandits/playlist_features.csv\"\n",
    "\n",
    "deezer_data = DeezerDataset(\n",
    "    user_features_path,\n",
    "    playlist_features_path,\n",
    "    len_list=12,\n",
    "    len_init=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating click probabilities: 100%|██████████| 100000/100000 [00:12<00:00, 7840.59it/s]\n",
      "Generating feedback: 100%|██████████| 100000/100000 [00:02<00:00, 42668.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cascade is enabled, so we observe at least 3 items per user per user session\n",
      "min number of actions is thus 100 batches * 1000 users * 3 items = 300,000\n",
      "feedback dict:\n",
      "  action: <class 'numpy.ndarray'>, (333027,)\n",
      "  reward: <class 'numpy.ndarray'>, (333027,)\n",
      "  position: <class 'numpy.ndarray'>, (333027,)\n",
      "  context: <class 'numpy.ndarray'>, (333027, 97)\n",
      "  action_context: <class 'numpy.ndarray'>, (333027, 97)\n",
      "  pscore: <class 'numpy.ndarray'>, (333027,)\n",
      "  n_rounds: 333027\n",
      "  n_actions: 862\n",
      "  users: <class 'numpy.ndarray'>, (100000,)\n",
      "  segments: <class 'numpy.ndarray'>, (333027,)\n"
     ]
    }
   ],
   "source": [
    "random_deezer_feedback = deezer_data.obtain_batch_bandit_feedback(\n",
    "    n_batches=100,\n",
    "    users_per_batch=1000,\n",
    "    cascade=True,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "print(\"\\ncascade is enabled, so we observe at least 3 items per user per user session\")\n",
    "print(\"min number of actions is thus 100 batches * 1000 users * 3 items = 300,000\")\n",
    "print(\"feedback dict:\")\n",
    "for key, value in random_deezer_feedback.items():\n",
    "    if key[0:2] != \"n_\":\n",
    "        print(f\"  {key}: {type(value)}, {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward for uniform random actions: 0.027\n"
     ]
    }
   ],
   "source": [
    "exp_rand_reward = round(random_deezer_feedback[\"reward\"].mean(),4)\n",
    "print(f\"Expected reward for uniform random actions: {exp_rand_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Do online bandit learning on context-free policies and segment-based policies\n",
    "\n",
    "Tried with 3 different policies options, each with two different parameter options (parameters are from Deezer paper). Each of the 6 policies is also tried as a segment-based policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_greedy_explore = EpsilonGreedy(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    # this batch_size setting will be ignored because supplying the policy\n",
    "    # to `deezer_data.obtain_batch_bandit_feedback` will manually update\n",
    "    # once per batch of *users*\n",
    "    batch_size=1, \n",
    "    random_state=1,\n",
    "    epsilon=0.1,\n",
    "    policy_name='e_greedy_explore'\n",
    ")\n",
    "\n",
    "e_greedy_exploit = EpsilonGreedy(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    # this batch_size setting will be ignored because supplying the policy\n",
    "    # to `deezer_data.obtain_batch_bandit_feedback` will manually update\n",
    "    # once per batch of *users*\n",
    "    batch_size=1, \n",
    "    random_state=1,\n",
    "    epsilon=0.01,\n",
    "    policy_name='e_greedy_exploit'\n",
    ")\n",
    "\n",
    "e_greedy_explore_seg = SegmentPolicy(e_greedy_explore, n_segments = 100)\n",
    "e_greedy_exploit_seg = SegmentPolicy(e_greedy_exploit, n_segments = 100)\n",
    "\n",
    "etc_explore = ExploreThenCommit(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    min_n=100,\n",
    "    policy_name='etc_explore'\n",
    ")\n",
    "\n",
    "etc_exploit = ExploreThenCommit(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    min_n=20,\n",
    "    policy_name='etc_exploit'\n",
    ")\n",
    "\n",
    "etc_explore_seg = SegmentPolicy(etc_explore, n_segments=100)\n",
    "etc_exploit_seg = SegmentPolicy(etc_exploit, n_segments=100)\n",
    "\n",
    "ts_naive = BernoulliTS(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    alpha=np.ones(deezer_data.n_actions),\n",
    "    beta=np.ones(deezer_data.n_actions),\n",
    "    policy_name='ts_naive'\n",
    ")\n",
    "\n",
    "ts_pessimistic = BernoulliTS(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    alpha=np.ones(deezer_data.n_actions),\n",
    "    beta=np.ones(deezer_data.n_actions)*99,\n",
    "    policy_name='ts_pessimistic')\n",
    "\n",
    "ts_naive_seg = SegmentPolicy(ts_naive, n_segments=100)\n",
    "ts_pessimistic_seg = SegmentPolicy(ts_pessimistic, n_segments=100)\n",
    "\n",
    "policies = [e_greedy_explore, e_greedy_exploit, \n",
    "            e_greedy_explore_seg, e_greedy_exploit_seg,\n",
    "            etc_explore, etc_exploit,\n",
    "            etc_explore_seg, etc_exploit_seg,\n",
    "            ts_naive, ts_pessimistic,\n",
    "            ts_naive_seg, ts_pessimistic_seg]\n",
    "\n",
    "policy_dict = [(policy.policy_name, policy) for policy in policies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning:   1%|          | 800/100000 [00:00<00:12, 7996.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:12<00:00, 8163.64it/s]\n",
      "Simulating online learning:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:11<00:00, 9074.40it/s]\n",
      "Simulating online learning:   1%|          | 723/100000 [00:00<00:13, 7224.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:14<00:00, 6836.19it/s]\n",
      "Simulating online learning:   1%|          | 777/100000 [00:00<00:12, 7769.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:15<00:00, 6448.12it/s]\n",
      "Simulating online learning:   1%|          | 662/100000 [00:00<00:15, 6618.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:20<00:00, 4897.25it/s]\n",
      "Simulating online learning:   1%|          | 600/100000 [00:00<00:16, 5995.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:18<00:00, 5482.75it/s]\n",
      "Simulating online learning:   1%|          | 645/100000 [00:00<00:15, 6449.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5115.56it/s]\n",
      "Simulating online learning:   1%|          | 547/100000 [00:00<00:18, 5469.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5223.44it/s]\n",
      "Simulating online learning:   0%|          | 476/100000 [00:00<00:20, 4756.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:23<00:00, 4189.89it/s]\n",
      "Simulating online learning:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:23<00:00, 4275.19it/s]\n",
      "Simulating online learning:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:25<00:00, 3867.49it/s]\n",
      "Simulating online learning:   0%|          | 362/100000 [00:00<00:27, 3617.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:25<00:00, 3949.46it/s]\n"
     ]
    }
   ],
   "source": [
    "feedback_dict = {}\n",
    "for policy in policies:\n",
    "    print(policy.policy_name)\n",
    "    feedback = deezer_data.obtain_batch_bandit_feedback(\n",
    "        policy=policy,\n",
    "        n_batches=100,\n",
    "        users_per_batch=1000,\n",
    "        cascade=True,\n",
    "        seed=1\n",
    "    )\n",
    "    feedback_dict[policy.policy_name] = feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've generated a dataset that contains the actions and rewards generated by an online experiment with our epsilon-greedy bandit.\n",
    "\n",
    "Using the `ReplayMethod` here isn't strictly necessary: since we did online learning, our logs always match our actions and so we could just as easily get `mean_eps_greedy_online_reward = eg_deezer_feedback[\"reward\"].mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward for e_greedy_explore trained online: 0.0614 (2.27x random baseline)!\n",
      "95% confidence interval is 0.0607-0.062\n",
      "\n",
      "Expected reward for e_greedy_exploit trained online: 0.1156 (4.28x random baseline)!\n",
      "95% confidence interval is 0.1146-0.1166\n",
      "\n",
      "Expected reward for e_greedy_explore_seg trained online: 0.0281 (1.04x random baseline)!\n",
      "95% confidence interval is 0.0276-0.0287\n",
      "\n",
      "Expected reward for e_greedy_exploit_seg trained online: 0.0282 (1.04x random baseline)!\n",
      "95% confidence interval is 0.0278-0.0287\n",
      "\n",
      "Expected reward for etc_explore trained online: 0.026 (0.96x random baseline)!\n",
      "95% confidence interval is 0.0255-0.0265\n",
      "\n",
      "Expected reward for etc_exploit trained online: 0.1874 (6.93x random baseline)!\n",
      "95% confidence interval is 0.1862-0.1886\n",
      "\n",
      "Expected reward for etc_explore_seg trained online: 0.0267 (0.99x random baseline)!\n",
      "95% confidence interval is 0.0262-0.0272\n",
      "\n",
      "Expected reward for etc_exploit_seg trained online: 0.0267 (0.99x random baseline)!\n",
      "95% confidence interval is 0.0261-0.0271\n",
      "\n",
      "Expected reward for ts_naive trained online: 0.2516 (9.31x random baseline)!\n",
      "95% confidence interval is 0.2504-0.253\n",
      "\n",
      "Expected reward for ts_pessimistic trained online: 0.2709 (10.02x random baseline)!\n",
      "95% confidence interval is 0.2694-0.2721\n",
      "\n",
      "Expected reward for ts_naive_seg trained online: 0.085 (3.14x random baseline)!\n",
      "95% confidence interval is 0.0841-0.086\n",
      "\n",
      "Expected reward for ts_pessimistic_seg trained online: 0.1853 (6.85x random baseline)!\n",
      "95% confidence interval is 0.1839-0.1866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_estimator = ReplayMethod()\n",
    "for policy_name in feedback_dict:\n",
    "    feedback = feedback_dict[policy_name]\n",
    "    estimates = replay_estimator.estimate_interval(\n",
    "        reward=feedback[\"reward\"],\n",
    "        action=feedback[\"action\"],\n",
    "        position=feedback[\"position\"],\n",
    "        action_dist=convert_to_action_dist(deezer_data.n_actions, feedback[\"selected_actions\"])\n",
    "    )\n",
    "\n",
    "    mean_reward = np.round(estimates[\"mean\"], 4)\n",
    "    online_relative = np.round(estimates[\"mean\"] / random_deezer_feedback[\"reward\"].mean(), 2)\n",
    "\n",
    "    print(f\"Expected reward for {policy_name} trained online: {mean_reward}\",\n",
    "          f\"({online_relative}x random baseline)!\")\n",
    "\n",
    "    lo_online_reward = np.round(estimates[\"95.0% CI (lower)\"], 4)\n",
    "    hi_online_reward = np.round(estimates[\"95.0% CI (upper)\"], 4)\n",
    "    print(f\"95% confidence interval is {lo_online_reward}-{hi_online_reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are interesting. The segment-based policies seem to almost always perform worse than the context-free ones.\n",
    "\n",
    "Now we can try updating parameters every round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:26<00:00, 3785.74it/s]\n",
      "Simulating online learning:   1%|          | 719/100000 [00:00<00:13, 7185.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:12<00:00, 8111.96it/s]\n",
      "Simulating online learning:   0%|          | 305/100000 [00:00<00:32, 3049.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:21<00:00, 4736.46it/s]\n",
      "Simulating online learning:   0%|          | 402/100000 [00:00<00:24, 4014.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5235.09it/s]\n",
      "Simulating online learning:   0%|          | 461/100000 [00:00<00:21, 4606.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5019.44it/s]\n",
      "Simulating online learning:   1%|          | 528/100000 [00:00<00:18, 5275.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5216.35it/s]\n",
      "Simulating online learning:   0%|          | 326/100000 [00:00<00:30, 3255.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:25<00:00, 3948.46it/s]\n",
      "Simulating online learning:   0%|          | 228/100000 [00:00<00:43, 2276.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:25<00:00, 3951.06it/s]\n",
      "Simulating online learning:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:25<00:00, 3964.85it/s]\n",
      "Simulating online learning:   0%|          | 342/100000 [00:00<00:29, 3418.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:25<00:00, 3959.49it/s]\n",
      "Simulating online learning:   0%|          | 170/100000 [00:00<00:58, 1697.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:31<00:00, 3195.65it/s]\n",
      "Simulating online learning:   0%|          | 123/100000 [00:00<01:21, 1227.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:30<00:00, 3229.34it/s]\n"
     ]
    }
   ],
   "source": [
    "feedback_dict = {}\n",
    "for policy in policies:\n",
    "    print(policy.policy_name)\n",
    "    feedback = deezer_data.obtain_batch_bandit_feedback(\n",
    "        policy=policy,\n",
    "        n_batches=100000,\n",
    "        users_per_batch=1,\n",
    "        cascade=True,\n",
    "        seed=1\n",
    "    )\n",
    "    feedback_dict[policy.policy_name] = feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward for e_greedy_explore trained online: 0.1053 (3.89x random baseline)!\n",
      "95% confidence interval is 0.1044-0.1063\n",
      "\n",
      "Expected reward for e_greedy_exploit trained online: 0.1337 (4.94x random baseline)!\n",
      "95% confidence interval is 0.1326-0.1347\n",
      "\n",
      "Expected reward for e_greedy_explore_seg trained online: 0.0432 (1.6x random baseline)!\n",
      "95% confidence interval is 0.0426-0.0439\n",
      "\n",
      "Expected reward for e_greedy_exploit_seg trained online: 0.0456 (1.69x random baseline)!\n",
      "95% confidence interval is 0.0449-0.0462\n",
      "\n",
      "Expected reward for etc_explore trained online: 0.2739 (10.13x random baseline)!\n",
      "95% confidence interval is 0.2726-0.2753\n",
      "\n",
      "Expected reward for etc_exploit trained online: 0.2815 (10.41x random baseline)!\n",
      "95% confidence interval is 0.2802-0.2831\n",
      "\n",
      "Expected reward for etc_explore_seg trained online: 0.0231 (0.86x random baseline)!\n",
      "95% confidence interval is 0.0227-0.0237\n",
      "\n",
      "Expected reward for etc_exploit_seg trained online: 0.0298 (1.1x random baseline)!\n",
      "95% confidence interval is 0.0292-0.0303\n",
      "\n",
      "Expected reward for ts_naive trained online: 0.2708 (10.02x random baseline)!\n",
      "95% confidence interval is 0.2694-0.2725\n",
      "\n",
      "Expected reward for ts_pessimistic trained online: 0.2749 (10.17x random baseline)!\n",
      "95% confidence interval is 0.2736-0.276\n",
      "\n",
      "Expected reward for ts_naive_seg trained online: 0.1577 (5.83x random baseline)!\n",
      "95% confidence interval is 0.1564-0.1588\n",
      "\n",
      "Expected reward for ts_pessimistic_seg trained online: 0.2684 (9.93x random baseline)!\n",
      "95% confidence interval is 0.2671-0.2698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_estimator = ReplayMethod()\n",
    "for policy_name in feedback_dict:\n",
    "    feedback = feedback_dict[policy_name]\n",
    "    estimates = replay_estimator.estimate_interval(\n",
    "        reward=feedback[\"reward\"],\n",
    "        action=feedback[\"action\"],\n",
    "        position=feedback[\"position\"],\n",
    "        action_dist=convert_to_action_dist(deezer_data.n_actions, feedback[\"selected_actions\"])\n",
    "    )\n",
    "\n",
    "    mean_reward = np.round(estimates[\"mean\"], 4)\n",
    "    online_relative = np.round(estimates[\"mean\"] / random_deezer_feedback[\"reward\"].mean(), 2)\n",
    "\n",
    "    print(f\"Expected reward for {policy_name} trained online: {mean_reward}\",\n",
    "          f\"({online_relative}x random baseline)!\")\n",
    "\n",
    "    lo_online_reward = np.round(estimates[\"95.0% CI (lower)\"], 4)\n",
    "    hi_online_reward = np.round(estimates[\"95.0% CI (upper)\"], 4)\n",
    "    print(f\"95% confidence interval is {lo_online_reward}-{hi_online_reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement for all policies except for etc_explore_seg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandits-env",
   "language": "python",
   "name": "bandits-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from obp.ope import ReplayMethod\n",
    "from obp.policy import EpsilonGreedy, BernoulliTS\n",
    "from sd_bandits.obp_extensions.policy import ExploreThenCommit, SegmentPolicy, KLUpperConfidenceBound\n",
    "from obp.simulator import run_bandit_simulation\n",
    "from obp.utils import convert_to_action_dist\n",
    "\n",
    "from sd_bandits.obp_extensions.dataset import DeezerDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Deezer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_path = \"../data/deezer_carousel_bandits/user_features.csv\"\n",
    "playlist_features_path = \"../data/deezer_carousel_bandits/playlist_features.csv\"\n",
    "\n",
    "deezer_data = DeezerDataset(\n",
    "    user_features_path,\n",
    "    playlist_features_path,\n",
    "    len_list=12,\n",
    "    len_init=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating click probabilities: 100%|██████████| 100000/100000 [00:12<00:00, 8139.69it/s]\n",
      "Generating feedback: 100%|██████████| 100000/100000 [00:02<00:00, 41458.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cascade is enabled, so we observe at least 3 items per user per user session\n",
      "min number of actions is thus 100 batches * 1000 users * 3 items = 300,000\n",
      "feedback dict:\n",
      "  action: <class 'numpy.ndarray'>, (333027,)\n",
      "  reward: <class 'numpy.ndarray'>, (333027,)\n",
      "  position: <class 'numpy.ndarray'>, (333027,)\n",
      "  context: <class 'numpy.ndarray'>, (333027, 97)\n",
      "  action_context: <class 'numpy.ndarray'>, (333027, 97)\n",
      "  pscore: <class 'numpy.ndarray'>, (333027,)\n",
      "  n_rounds: 333027\n",
      "  n_actions: 862\n",
      "  users: <class 'numpy.ndarray'>, (100000,)\n",
      "  segments: <class 'numpy.ndarray'>, (333027,)\n",
      "  batches: <class 'numpy.ndarray'>, (333027,)\n"
     ]
    }
   ],
   "source": [
    "random_deezer_feedback = deezer_data.obtain_batch_bandit_feedback(\n",
    "    n_batches=100,\n",
    "    users_per_batch=1000,\n",
    "    cascade=True,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "print(\"\\ncascade is enabled, so we observe at least 3 items per user per user session\")\n",
    "print(\"min number of actions is thus 100 batches * 1000 users * 3 items = 300,000\")\n",
    "print(\"feedback dict:\")\n",
    "for key, value in random_deezer_feedback.items():\n",
    "    if key[0:2] != \"n_\":\n",
    "        print(f\"  {key}: {type(value)}, {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward for uniform random actions: 0.027\n"
     ]
    }
   ],
   "source": [
    "exp_rand_reward = round(random_deezer_feedback[\"reward\"].mean(),4)\n",
    "print(f\"Expected reward for uniform random actions: {exp_rand_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Do online bandit learning on context-free policies and segment-based policies\n",
    "\n",
    "Tried with 3 different policies options, each with two different parameter options (parameters are from Deezer paper). Each of the 6 policies is also tried as a segment-based policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_greedy_explore = EpsilonGreedy(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    # this batch_size setting will be ignored because supplying the policy\n",
    "    # to `deezer_data.obtain_batch_bandit_feedback` will manually update\n",
    "    # once per batch of *users*\n",
    "    batch_size=1, \n",
    "    random_state=1,\n",
    "    epsilon=0.1,\n",
    "    policy_name='e_greedy_explore'\n",
    ")\n",
    "\n",
    "e_greedy_exploit = EpsilonGreedy(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    # this batch_size setting will be ignored because supplying the policy\n",
    "    # to `deezer_data.obtain_batch_bandit_feedback` will manually update\n",
    "    # once per batch of *users*\n",
    "    batch_size=1, \n",
    "    random_state=1,\n",
    "    epsilon=0.01,\n",
    "    policy_name='e_greedy_exploit'\n",
    ")\n",
    "\n",
    "e_greedy_explore_seg = SegmentPolicy(e_greedy_explore, n_segments = 100)\n",
    "e_greedy_exploit_seg = SegmentPolicy(e_greedy_exploit, n_segments = 100)\n",
    "\n",
    "etc_explore = ExploreThenCommit(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    min_n=100,\n",
    "    policy_name='etc_explore'\n",
    ")\n",
    "\n",
    "etc_exploit = ExploreThenCommit(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    min_n=20,\n",
    "    policy_name='etc_exploit'\n",
    ")\n",
    "\n",
    "etc_explore_seg = SegmentPolicy(etc_explore, n_segments=100)\n",
    "etc_exploit_seg = SegmentPolicy(etc_exploit, n_segments=100)\n",
    "\n",
    "ts_naive = BernoulliTS(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    alpha=np.ones(deezer_data.n_actions),\n",
    "    beta=np.ones(deezer_data.n_actions),\n",
    "    policy_name='ts_naive'\n",
    ")\n",
    "\n",
    "ts_pessimistic = BernoulliTS(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list=12,\n",
    "    batch_size=1,\n",
    "    random_state=1,\n",
    "    alpha=np.ones(deezer_data.n_actions),\n",
    "    beta=np.ones(deezer_data.n_actions)*99,\n",
    "    policy_name='ts_pessimistic')\n",
    "\n",
    "ts_naive_seg = SegmentPolicy(ts_naive, n_segments=100)\n",
    "ts_pessimistic_seg = SegmentPolicy(ts_pessimistic, n_segments=100)\n",
    "\n",
    "kl_ucb = KLUpperConfidenceBound(\n",
    "    n_actions=deezer_data.n_actions,\n",
    "    len_list = deezer_data.len_list,\n",
    "    batch_size=1000,\n",
    "    random_state=0,\n",
    ")\n",
    "kl_ucb_seg = SegmentPolicy(kl_ucb, n_segments=100)\n",
    "\n",
    "policies = [e_greedy_explore, e_greedy_exploit, \n",
    "            e_greedy_explore_seg, e_greedy_exploit_seg,\n",
    "            etc_explore, etc_exploit,\n",
    "            etc_explore_seg, etc_exploit_seg,\n",
    "            ts_naive, ts_pessimistic,\n",
    "            ts_naive_seg, ts_pessimistic_seg,\n",
    "            kl_ucb, kl_ucb_seg]\n",
    "\n",
    "policy_dict = dict([(policy.policy_name, policy) for policy in policies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simulating online learning:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:13<00:00, 7471.21it/s]\n",
      "Simulating online learning:   1%|          | 801/100000 [00:00<00:12, 8005.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:12<00:00, 8278.22it/s]\n",
      "Simulating online learning:   1%|          | 736/100000 [00:00<00:13, 7353.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:14<00:00, 6734.06it/s]\n",
      "Simulating online learning:   1%|          | 730/100000 [00:00<00:13, 7292.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:15<00:00, 6649.59it/s]\n",
      "Simulating online learning:   1%|          | 639/100000 [00:00<00:15, 6386.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:20<00:00, 4790.89it/s]\n",
      "Simulating online learning:   1%|          | 675/100000 [00:00<00:14, 6744.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:18<00:00, 5359.70it/s]\n",
      "Simulating online learning:   1%|          | 614/100000 [00:00<00:16, 6134.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5020.98it/s]\n",
      "Simulating online learning:   1%|          | 631/100000 [00:00<00:15, 6302.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5078.01it/s]\n",
      "Simulating online learning:   0%|          | 468/100000 [00:00<00:21, 4679.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:24<00:00, 4027.85it/s]\n",
      "Simulating online learning:   0%|          | 342/100000 [00:00<00:29, 3419.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:25<00:00, 3892.46it/s]\n",
      "Simulating online learning:   0%|          | 370/100000 [00:00<00:26, 3699.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:28<00:00, 3497.38it/s]\n",
      "Simulating online learning:   0%|          | 387/100000 [00:00<00:25, 3867.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:27<00:00, 3610.35it/s]\n",
      "Simulating online learning:   0%|          | 215/100000 [00:00<00:46, 2147.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_ucb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:47<00:00, 2105.52it/s]\n",
      "Simulating online learning:   1%|          | 623/100000 [00:00<00:15, 6227.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_ucb_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:45<00:00, 2196.52it/s]\n"
     ]
    }
   ],
   "source": [
    "feedback_dict = {}\n",
    "for policy in policies:\n",
    "    print(policy.policy_name)\n",
    "    feedback = deezer_data.obtain_batch_bandit_feedback(\n",
    "        policy=policy,\n",
    "        n_batches=100,\n",
    "        users_per_batch=1000,\n",
    "        cascade=True,\n",
    "        seed=1\n",
    "    )\n",
    "    feedback_dict[policy.policy_name] = feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've generated a dataset that contains the actions and rewards generated by an online experiment with our epsilon-greedy bandit.\n",
    "\n",
    "Using the `ReplayMethod` here isn't strictly necessary: since we did online learning, our logs always match our actions and so we could just as easily get `mean_eps_greedy_online_reward = eg_deezer_feedback[\"reward\"].mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward for e_greedy_explore trained online: 0.0613 (2.27x random baseline)!\n",
      "95% confidence interval is 0.0605-0.0621\n",
      "\n",
      "Expected reward for e_greedy_exploit trained online: 0.1157 (4.28x random baseline)!\n",
      "95% confidence interval is 0.1149-0.1166\n",
      "\n",
      "Expected reward for e_greedy_explore_seg trained online: 0.0281 (1.04x random baseline)!\n",
      "95% confidence interval is 0.0276-0.0287\n",
      "\n",
      "Expected reward for e_greedy_exploit_seg trained online: 0.0283 (1.05x random baseline)!\n",
      "95% confidence interval is 0.0278-0.0289\n",
      "\n",
      "Expected reward for etc_explore trained online: 0.0261 (0.96x random baseline)!\n",
      "95% confidence interval is 0.0256-0.0266\n",
      "\n",
      "Expected reward for etc_exploit trained online: 0.1874 (6.93x random baseline)!\n",
      "95% confidence interval is 0.1859-0.1885\n",
      "\n",
      "Expected reward for etc_explore_seg trained online: 0.0267 (0.99x random baseline)!\n",
      "95% confidence interval is 0.0262-0.0272\n",
      "\n",
      "Expected reward for etc_exploit_seg trained online: 0.0267 (0.99x random baseline)!\n",
      "95% confidence interval is 0.0262-0.0271\n",
      "\n",
      "Expected reward for ts_naive trained online: 0.2516 (9.31x random baseline)!\n",
      "95% confidence interval is 0.2501-0.253\n",
      "\n",
      "Expected reward for ts_pessimistic trained online: 0.271 (10.02x random baseline)!\n",
      "95% confidence interval is 0.2699-0.2723\n",
      "\n",
      "Expected reward for ts_naive_seg trained online: 0.0851 (3.15x random baseline)!\n",
      "95% confidence interval is 0.0843-0.0859\n",
      "\n",
      "Expected reward for ts_pessimistic_seg trained online: 0.1853 (6.85x random baseline)!\n",
      "95% confidence interval is 0.1841-0.1864\n",
      "\n",
      "Expected reward for kl_ucb trained online: 0.0312 (1.15x random baseline)!\n",
      "95% confidence interval is 0.0306-0.0317\n",
      "\n",
      "Expected reward for kl_ucb_seg trained online: 0.1034 (3.82x random baseline)!\n",
      "95% confidence interval is 0.1024-0.1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_estimator = ReplayMethod()\n",
    "for policy_name in feedback_dict:\n",
    "    feedback = feedback_dict[policy_name]\n",
    "    estimates = replay_estimator.estimate_interval(\n",
    "        reward=feedback[\"reward\"],\n",
    "        action=feedback[\"action\"],\n",
    "        position=feedback[\"position\"],\n",
    "        action_dist=convert_to_action_dist(deezer_data.n_actions, feedback[\"selected_actions\"])\n",
    "    )\n",
    "\n",
    "    mean_reward = np.round(estimates[\"mean\"], 4)\n",
    "    online_relative = np.round(estimates[\"mean\"] / random_deezer_feedback[\"reward\"].mean(), 2)\n",
    "\n",
    "    print(f\"Expected reward for {policy_name} trained online: {mean_reward}\",\n",
    "          f\"({online_relative}x random baseline)!\")\n",
    "\n",
    "    lo_online_reward = np.round(estimates[\"95.0% CI (lower)\"], 4)\n",
    "    hi_online_reward = np.round(estimates[\"95.0% CI (upper)\"], 4)\n",
    "    print(f\"95% confidence interval is {lo_online_reward}-{hi_online_reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are interesting. The segment-based policies seem to almost always perform worse than the context-free ones.\n",
    "\n",
    "Now we can try updating parameters every round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning:   0%|          | 324/100000 [00:00<00:30, 3234.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:22<00:00, 4409.28it/s]\n",
      "Simulating online learning:   0%|          | 375/100000 [00:00<00:26, 3743.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:21<00:00, 4672.39it/s]\n",
      "Simulating online learning:   0%|          | 298/100000 [00:00<00:33, 2975.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:29<00:00, 3375.80it/s]\n",
      "Simulating online learning:   0%|          | 176/100000 [00:00<00:56, 1752.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_greedy_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:42<00:00, 2358.87it/s]\n",
      "Simulating online learning:   0%|          | 227/100000 [00:00<00:44, 2265.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:28<00:00, 3501.78it/s]\n",
      "Simulating online learning:   0%|          | 307/100000 [00:00<00:32, 3069.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:26<00:00, 3802.88it/s]\n",
      "Simulating online learning:   0%|          | 152/100000 [00:00<01:05, 1517.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_explore_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:35<00:00, 2805.83it/s]\n",
      "Simulating online learning:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etc_exploit_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:36<00:00, 2727.87it/s]\n",
      "Simulating online learning:   0%|          | 172/100000 [00:00<00:58, 1716.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:41<00:00, 2405.25it/s]\n",
      "Simulating online learning:   0%|          | 282/100000 [00:00<00:35, 2816.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:36<00:00, 2708.62it/s]\n",
      "Simulating online learning:   0%|          | 91/100000 [00:00<01:50, 905.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_naive_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:48<00:00, 2049.66it/s]\n",
      "Simulating online learning:   0%|          | 115/100000 [00:00<01:27, 1141.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_pessimistic_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:56<00:00, 1766.38it/s]\n",
      "Simulating online learning:   0%|          | 109/100000 [00:00<01:32, 1082.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_ucb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [01:32<00:00, 1084.80it/s]\n",
      "Simulating online learning:   0%|          | 23/100000 [00:00<07:21, 226.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_ucb_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [01:20<00:00, 1243.42it/s]\n"
     ]
    }
   ],
   "source": [
    "feedback_dict = {}\n",
    "for policy in policies:\n",
    "    print(policy.policy_name)\n",
    "    feedback = deezer_data.obtain_batch_bandit_feedback(\n",
    "        policy=policy,\n",
    "        n_batches=100000,\n",
    "        users_per_batch=1,\n",
    "        cascade=True,\n",
    "        seed=1\n",
    "    )\n",
    "    feedback_dict[policy.policy_name] = feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward for e_greedy_explore trained online: 0.0849 (3.14x random baseline)!\n",
      "95% confidence interval is 0.0841-0.0855\n",
      "\n",
      "Expected reward for e_greedy_exploit trained online: 0.1155 (4.27x random baseline)!\n",
      "95% confidence interval is 0.1145-0.1165\n",
      "\n",
      "Expected reward for e_greedy_explore_seg trained online: 0.0828 (3.06x random baseline)!\n",
      "95% confidence interval is 0.0817-0.0839\n",
      "\n",
      "Expected reward for e_greedy_exploit_seg trained online: 0.0488 (1.8x random baseline)!\n",
      "95% confidence interval is 0.048-0.0494\n",
      "\n",
      "Expected reward for etc_explore trained online: 0.2739 (10.13x random baseline)!\n",
      "95% confidence interval is 0.2723-0.2754\n",
      "\n",
      "Expected reward for etc_exploit trained online: 0.2815 (10.41x random baseline)!\n",
      "95% confidence interval is 0.28-0.2829\n",
      "\n",
      "Expected reward for etc_explore_seg trained online: 0.0231 (0.86x random baseline)!\n",
      "95% confidence interval is 0.0226-0.0236\n",
      "\n",
      "Expected reward for etc_exploit_seg trained online: 0.0298 (1.1x random baseline)!\n",
      "95% confidence interval is 0.0293-0.0304\n",
      "\n",
      "Expected reward for ts_naive trained online: 0.2707 (10.01x random baseline)!\n",
      "95% confidence interval is 0.2695-0.2721\n",
      "\n",
      "Expected reward for ts_pessimistic trained online: 0.2746 (10.16x random baseline)!\n",
      "95% confidence interval is 0.2733-0.2758\n",
      "\n",
      "Expected reward for ts_naive_seg trained online: 0.1577 (5.83x random baseline)!\n",
      "95% confidence interval is 0.1563-0.1587\n",
      "\n",
      "Expected reward for ts_pessimistic_seg trained online: 0.2684 (9.93x random baseline)!\n",
      "95% confidence interval is 0.2668-0.2698\n",
      "\n",
      "Expected reward for kl_ucb trained online: 0.2565 (9.49x random baseline)!\n",
      "95% confidence interval is 0.255-0.2582\n",
      "\n",
      "Expected reward for kl_ucb_seg trained online: 0.1165 (4.31x random baseline)!\n",
      "95% confidence interval is 0.1155-0.1177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_estimator = ReplayMethod()\n",
    "for policy_name in feedback_dict:\n",
    "    feedback = feedback_dict[policy_name]\n",
    "    estimates = replay_estimator.estimate_interval(\n",
    "        reward=feedback[\"reward\"],\n",
    "        action=feedback[\"action\"],\n",
    "        position=feedback[\"position\"],\n",
    "        action_dist=convert_to_action_dist(deezer_data.n_actions, feedback[\"selected_actions\"])\n",
    "    )\n",
    "\n",
    "    mean_reward = np.round(estimates[\"mean\"], 4)\n",
    "    online_relative = np.round(estimates[\"mean\"] / random_deezer_feedback[\"reward\"].mean(), 2)\n",
    "\n",
    "    print(f\"Expected reward for {policy_name} trained online: {mean_reward}\",\n",
    "          f\"({online_relative}x random baseline)!\")\n",
    "\n",
    "    lo_online_reward = np.round(estimates[\"95.0% CI (lower)\"], 4)\n",
    "    hi_online_reward = np.round(estimates[\"95.0% CI (upper)\"], 4)\n",
    "    print(f\"95% confidence interval is {lo_online_reward}-{hi_online_reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement for all policies except for etc_explore_seg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandits-env",
   "language": "python",
   "name": "bandits-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

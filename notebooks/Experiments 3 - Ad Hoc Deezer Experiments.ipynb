{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from obp.dataset.real import OpenBanditDataset\n",
    "from obp.policy import EpsilonGreedy, BernoulliTS, Random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sd_bandits.obp_extensions.dataset import DeezerDataset\n",
    "from sd_bandits.obp_extensions.policy import ExploreThenCommit, SegmentPolicy\n",
    "from sd_bandits.experiment import DeezerExperiment, OBDExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    "    datefmt=\"%-I:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad hoc deezer experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_dataset = DeezerDataset(\"../data/deezer_carousel_bandits/user_features.csv\",\"../data/deezer_carousel_bandits/playlist_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    Random(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.01,\n",
    "        policy_name=\"egreedy_exploit\",\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.1,\n",
    "        policy_name=\"egreedy_explore\",\n",
    "    ),\n",
    "    BernoulliTS(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        alpha=1,\n",
    "        beta=1,\n",
    "        policy_name=\"ts_naive\",\n",
    "    ),\n",
    "    BernoulliTS(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        alpha=100,\n",
    "        beta=100,\n",
    "        policy_name=\"ts_pessimistic\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        EpsilonGreedy(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=0,\n",
    "            epsilon=0.1,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_egreedy_explore\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        EpsilonGreedy(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=0,\n",
    "            epsilon=0.01,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_egreedy_exploit\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        BernoulliTS(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=0,\n",
    "            alpha=1,\n",
    "            beta=1,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_ts_naive\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        BernoulliTS(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=0,\n",
    "            alpha=100,\n",
    "            beta=100,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_ts_pessimistic\",\n",
    "    ),\n",
    "    ExploreThenCommit(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        min_n=20,\n",
    "        policy_name=\"etc_exploit\",\n",
    "    ),\n",
    "    ExploreThenCommit(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        min_n=100,\n",
    "        policy_name=\"etc_explore\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        ExploreThenCommit(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=0,\n",
    "            min_n=20,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_etc_exploit\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        ExploreThenCommit(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=0,\n",
    "            min_n=100,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_etc_explore\",\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_experiment = DeezerExperiment(\n",
    "    dataset=deezer_dataset,\n",
    "    policies=[(policy, {\"users_per_batch\": 1000}) for policy in policies],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:24:14 INFO: Running experiment\n",
      "11:24:14 INFO: Learning and obtaining policy feedback\n",
      "11:24:14 INFO: [1 of 13] Learning and obtaining random feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:18<00:00, 5448.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:24:34 INFO: [2 of 13] Learning and obtaining egreedy_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:16<00:00, 5909.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:24:53 INFO: [3 of 13] Learning and obtaining egreedy_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:17<00:00, 5599.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:25:12 INFO: [4 of 13] Learning and obtaining ts_naive feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:29<00:00, 3342.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:25:44 INFO: [5 of 13] Learning and obtaining ts_pessimistic feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:31<00:00, 3159.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:26:17 INFO: [6 of 13] Learning and obtaining seg_egreedy_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:21<00:00, 4679.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:26:40 INFO: [7 of 13] Learning and obtaining seg_egreedy_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:21<00:00, 4671.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:27:03 INFO: [8 of 13] Learning and obtaining seg_ts_naive feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:33<00:00, 2961.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:27:38 INFO: [9 of 13] Learning and obtaining seg_ts_pessimistic feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:33<00:00, 2944.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:28:14 INFO: [10 of 13] Learning and obtaining etc_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:26<00:00, 3757.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:28:42 INFO: [11 of 13] Learning and obtaining etc_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:29<00:00, 3413.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:29:12 INFO: [12 of 13] Learning and obtaining seg_etc_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:32<00:00, 3108.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:29:48 INFO: [13 of 13] Learning and obtaining seg_etc_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:30<00:00, 3285.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:30:20 INFO: Done in 365.3 seconds\n",
      "11:30:20 INFO: Estimating reward confidence interval for random baseline feedback\n",
      "11:30:20 INFO: [1 of 13] Estimating reward confindence interval for random feedback\n",
      "11:30:21 INFO: [2 of 13] Estimating reward confindence interval for egreedy_exploit feedback\n",
      "11:30:21 INFO: [3 of 13] Estimating reward confindence interval for egreedy_explore feedback\n",
      "11:30:22 INFO: [4 of 13] Estimating reward confindence interval for ts_naive feedback\n",
      "11:30:23 INFO: [5 of 13] Estimating reward confindence interval for ts_pessimistic feedback\n",
      "11:30:23 INFO: [6 of 13] Estimating reward confindence interval for seg_egreedy_explore feedback\n",
      "11:30:24 INFO: [7 of 13] Estimating reward confindence interval for seg_egreedy_exploit feedback\n",
      "11:30:25 INFO: [8 of 13] Estimating reward confindence interval for seg_ts_naive feedback\n",
      "11:30:26 INFO: [9 of 13] Estimating reward confindence interval for seg_ts_pessimistic feedback\n",
      "11:30:26 INFO: [10 of 13] Estimating reward confindence interval for etc_exploit feedback\n",
      "11:30:27 INFO: [11 of 13] Estimating reward confindence interval for etc_explore feedback\n",
      "11:30:28 INFO: [12 of 13] Estimating reward confindence interval for seg_etc_exploit feedback\n",
      "11:30:28 INFO: [13 of 13] Estimating reward confindence interval for seg_etc_explore feedback\n",
      "11:30:29 INFO: Done in 9.29 seconds\n",
      "11:30:29 INFO: Experiment finished in 374.59 seconds\n"
     ]
    }
   ],
   "source": [
    "deezer_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../experiments/ad_hoc/most_deezer.pickle\",\"wb\") as f:\n",
    "    pickle.dump(deezer_experiment.output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from obp.policy import EpsilonGreedy, BernoulliTS, Random, LinEpsilonGreedy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sd_bandits.experiment import DeezerExperiment\n",
    "from sd_bandits.obp_extensions.dataset import DeezerDataset\n",
    "from sd_bandits.obp_extensions.policy import ExploreThenCommit, KLUpperConfidenceBound, SegmentPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    "    datefmt=\"%-I:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad hoc deezer experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_dataset = DeezerDataset(\n",
    "    \"../data/deezer_carousel_bandits/user_features.csv\",\n",
    "    \"../data/deezer_carousel_bandits/playlist_features.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    Random(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        epsilon=0.01,\n",
    "        policy_name=\"egreedy_exploit\",\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        epsilon=0.1,\n",
    "        policy_name=\"egreedy_explore\",\n",
    "    ),\n",
    "    BernoulliTS(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        alpha=1,\n",
    "        beta=1,\n",
    "        policy_name=\"ts_naive\",\n",
    "    ),\n",
    "    BernoulliTS(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        alpha=1,\n",
    "        beta=100,\n",
    "        policy_name=\"ts_pessimistic\",\n",
    "    ),\n",
    "    ExploreThenCommit(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        min_n=20,\n",
    "        policy_name=\"etc_exploit\",\n",
    "    ),\n",
    "    ExploreThenCommit(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        min_n=100,\n",
    "        policy_name=\"etc_explore\",\n",
    "    ),\n",
    "    LinEpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        dim=96,\n",
    "        epsilon=0.01,\n",
    "        # policy_name=\"lin_egreedy_explore\",\n",
    "    ),\n",
    "    LinEpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        dim=96,\n",
    "        epsilon=0.1,\n",
    "        # policy_name=\"lin_egreedy_exploit\",\n",
    "    ),\n",
    "    KLUpperConfidenceBound(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        policy_name=\"kl_ucp\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        EpsilonGreedy(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            epsilon=0.01,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_egreedy_exploit\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        EpsilonGreedy(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            epsilon=0.1,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_egreedy_explore\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        BernoulliTS(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            alpha=1,\n",
    "            beta=1,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_ts_naive\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        BernoulliTS(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            alpha=1,\n",
    "            beta=100,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_ts_pessimistic\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        ExploreThenCommit(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            min_n=20,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_etc_exploit\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        ExploreThenCommit(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            min_n=100,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_etc_explore\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        KLUpperConfidenceBound(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            policy_name=\"kl_ucp\",\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_kl_ucp\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_experiment = DeezerExperiment(\n",
    "    dataset=deezer_dataset,\n",
    "    policies=[\n",
    "        (policy, {\"users_per_batch\": 20000})\n",
    "        if policy.policy_name[:3] != \"lin\"\n",
    "        else (policy, {\"users_per_batch\": 5000})\n",
    "        for policy in policies\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Accidentally ran with linear dimension set to 96 instead of 97; so stopped early and reran the rest as a separate experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:41:51 INFO: Running experiment\n",
      "6:41:51 INFO: Learning and obtaining policy feedback\n",
      "6:41:51 INFO: [1 of 17] Learning and obtaining random feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [06:05<00:00, 5471.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:48:57 INFO: [2 of 17] Learning and obtaining egreedy_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [06:22<00:00, 5232.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:56:17 INFO: [3 of 17] Learning and obtaining egreedy_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [06:33<00:00, 5077.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:03:42 INFO: [4 of 17] Learning and obtaining ts_naive feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [11:26<00:00, 2912.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:18:52 INFO: [5 of 17] Learning and obtaining ts_pessimistic feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [10:51<00:00, 3071.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:30:34 INFO: [6 of 17] Learning and obtaining etc_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [09:20<00:00, 3567.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:40:59 INFO: [7 of 17] Learning and obtaining etc_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [10:15<00:00, 3246.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:52:07 INFO: [8 of 17] Learning and obtaining linear_epsilon_greedy_0.01 feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning:   0%|          | 0/500000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 96 is different from 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6d56f07a42e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeezer_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/experiment.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running experiment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Experiment finished in {round(toc - tic, 2)} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/experiment.py\u001b[0m in \u001b[0;36m_run_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_and_obtain_policy_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_policy_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/utils.py\u001b[0m in \u001b[0;36mperformance_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mperformance_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Done in {round(toc - tic, 2)} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/experiment.py\u001b[0m in \u001b[0;36mlearn_and_obtain_policy_feedback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m                     \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                     \u001b[0mlightweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0mpolicy_meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 )\n\u001b[1;32m    257\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/obp_extensions/dataset.py\u001b[0m in \u001b[0;36mobtain_batch_bandit_feedback\u001b[0;34m(self, n_batches, users_per_batch, seed, cascade, cascade_at, policy, lightweight)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mcascade\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mcascade_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mlightweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             )\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/obp_extensions/dataset.py\u001b[0m in \u001b[0;36m_obtain_batch_bandit_feedback_on_policy\u001b[0;34m(self, policy, n_batches, users_per_batch, seed, cascade, cascade_at, lightweight)\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mitem_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"contextual\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                 \u001b[0mitem_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"segmented\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mitem_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_segments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/envs/sd_bandits/lib/python3.7/site-packages/obp/policy/linear.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             )  # dim * n_actions\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mpredicted_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 96 is different from 97)"
     ]
    }
   ],
   "source": [
    "deezer_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:54:45 INFO: Estimating reward confidence interval for random baseline feedback\n",
      "7:54:45 INFO: [1 of 17] Estimating reward confindence interval for random feedback\n",
      "7:55:00 INFO: [2 of 17] Estimating reward confindence interval for egreedy_exploit feedback\n",
      "7:55:18 INFO: [3 of 17] Estimating reward confindence interval for egreedy_explore feedback\n",
      "7:55:37 INFO: [4 of 17] Estimating reward confindence interval for ts_naive feedback\n",
      "7:55:55 INFO: [5 of 17] Estimating reward confindence interval for ts_pessimistic feedback\n",
      "7:56:13 INFO: [6 of 17] Estimating reward confindence interval for etc_exploit feedback\n",
      "7:56:32 INFO: [7 of 17] Estimating reward confindence interval for etc_explore feedback\n",
      "7:56:47 INFO: [8 of 17] Estimating reward confindence interval for linear_epsilon_greedy_0.01 feedback\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3094c7f3281c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeezer_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_policy_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/utils.py\u001b[0m in \u001b[0;36mperformance_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mperformance_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Done in {round(toc - tic, 2)} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (Personal)/Grad School/Search and Discovery/sd_bandits/sd_bandits/experiment.py\u001b[0m in \u001b[0;36mget_policy_rewards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_confidence_interval_by_bootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_feedback\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mn_bootstrap_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reward'"
     ]
    }
   ],
   "source": [
    "deezer_experiment.get_policy_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../results_deezer_1_7.pickle\",\"wb\") as f:\n",
    "    pickle.dump(deezer_experiment.output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy_feedback': {'random': {'reward': array([0, 0, 0, ..., 0, 0, 0]),\n",
       "   'n_rounds': 6649827,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'egreedy_exploit': {'reward': array([0, 0, 0, ..., 0, 0, 0]),\n",
       "   'n_rounds': 6980548,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'egreedy_explore': {'reward': array([0, 0, 0, ..., 0, 0, 0]),\n",
       "   'n_rounds': 7269248,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'ts_naive': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 6883744,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'ts_pessimistic': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 7232567,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'etc_exploit': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 7158061,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'etc_explore': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 6724919,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'linear_epsilon_greedy_0.01': {}},\n",
       " 'policies': [(Random(n_actions=862, len_list=12, batch_size=6649827, random_state=1, epsilon=1.0, policy_name='random'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (EpsilonGreedy(n_actions=862, len_list=12, batch_size=6980548, random_state=1, epsilon=0.01, policy_name='egreedy_exploit'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (EpsilonGreedy(n_actions=862, len_list=12, batch_size=7269248, random_state=1, epsilon=0.1, policy_name='egreedy_explore'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (BernoulliTS(n_actions=862, len_list=12, batch_size=6883744, random_state=1, alpha=1, beta=1, is_zozotown_prior=False, campaign=None, policy_name='ts_naive'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (BernoulliTS(n_actions=862, len_list=12, batch_size=7232567, random_state=1, alpha=1, beta=100, is_zozotown_prior=False, campaign=None, policy_name='ts_pessimistic'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (ExploreThenCommit(n_actions=862, len_list=12, batch_size=7158061, random_state=1, min_n=20, policy_name='etc_exploit'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (ExploreThenCommit(n_actions=862, len_list=12, batch_size=6724919, random_state=1, min_n=100, policy_name='etc_explore'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (LinEpsilonGreedy(dim=96, n_actions=862, len_list=12, batch_size=inf, alpha_=1.0, lambda_=1.0, random_state=1, epsilon=0.01),\n",
       "   {'users_per_batch': 5000}),\n",
       "  (LinEpsilonGreedy(dim=96, n_actions=862, len_list=12, batch_size=1, alpha_=1.0, lambda_=1.0, random_state=1, epsilon=0.1),\n",
       "   {'users_per_batch': 5000}),\n",
       "  (KLUpperConfidenceBound(n_actions=862, len_list=12, batch_size=1, random_state=1, precision=1e-06, eps=1e-15, policy_name='kl_ucp'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=EpsilonGreedy(n_actions=862, len_list=12, batch_size=1, random_state=1, epsilon=0.01, policy_name='egreedy_1.0'), n_segments=100, policy_name='seg_egreedy_exploit'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=EpsilonGreedy(n_actions=862, len_list=12, batch_size=1, random_state=1, epsilon=0.1, policy_name='egreedy_1.0'), n_segments=100, policy_name='seg_egreedy_explore'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=BernoulliTS(n_actions=862, len_list=12, batch_size=1, random_state=1, alpha=1, beta=1, is_zozotown_prior=False, campaign=None, policy_name='bts'), n_segments=100, policy_name='seg_ts_naive'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=BernoulliTS(n_actions=862, len_list=12, batch_size=1, random_state=1, alpha=1, beta=100, is_zozotown_prior=False, campaign=None, policy_name='bts'), n_segments=100, policy_name='seg_ts_pessimistic'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=ExploreThenCommit(n_actions=862, len_list=12, batch_size=1, random_state=1, min_n=20, policy_name='etc_20'), n_segments=100, policy_name='seg_etc_exploit'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=ExploreThenCommit(n_actions=862, len_list=12, batch_size=1, random_state=1, min_n=100, policy_name='etc_100'), n_segments=100, policy_name='seg_etc_explore'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=KLUpperConfidenceBound(n_actions=862, len_list=12, batch_size=1, random_state=1, precision=1e-06, eps=1e-15, policy_name='kl_ucp'), n_segments=100, policy_name='seg_kl_ucp'),\n",
       "   {'users_per_batch': 20000})],\n",
       " 'reward_summary': {'random': {'mean': 0.026799067103550213,\n",
       "   '95.0% CI (lower)': 0.02668287084761754,\n",
       "   '95.0% CI (upper)': 0.02691145574163057},\n",
       "  'egreedy_exploit': {'mean': 0.03723805065161074,\n",
       "   '95.0% CI (lower)': 0.037114081158098194,\n",
       "   '95.0% CI (upper)': 0.037369576858435755},\n",
       "  'egreedy_explore': {'mean': 0.04577730736384287,\n",
       "   '95.0% CI (lower)': 0.04563317622400556,\n",
       "   '95.0% CI (upper)': 0.04593477206995827},\n",
       "  'ts_naive': {'mean': 0.26757511319421523,\n",
       "   '95.0% CI (lower)': 0.2673256697227555,\n",
       "   '95.0% CI (upper)': 0.26787600977026454},\n",
       "  'ts_pessimistic': {'mean': 0.26952728401962955,\n",
       "   '95.0% CI (lower)': 0.26917738197240343,\n",
       "   '95.0% CI (upper)': 0.2699144992365781},\n",
       "  'etc_exploit': {'mean': 0.26667064865750656,\n",
       "   '95.0% CI (lower)': 0.2663522356124096,\n",
       "   '95.0% CI (upper)': 0.26700129406552975},\n",
       "  'etc_explore': {'mean': 0.07716605954659082,\n",
       "   '95.0% CI (lower)': 0.07694356244290823,\n",
       "   '95.0% CI (upper)': 0.07736587236218012}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deezer_experiment.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Resuming from where we left off, but this time with correct dimension for linear policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_cont = [\n",
    "    LinEpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        dim=97,\n",
    "        epsilon=0.01,\n",
    "        # policy_name=\"lin_egreedy_explore\",\n",
    "    ),\n",
    "    LinEpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        dim=97,\n",
    "        epsilon=0.1,\n",
    "        # policy_name=\"lin_egreedy_exploit\",\n",
    "    ),\n",
    "    KLUpperConfidenceBound(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        policy_name=\"kl_ucp\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        EpsilonGreedy(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            epsilon=0.01,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_egreedy_exploit\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        EpsilonGreedy(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            epsilon=0.1,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_egreedy_explore\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        BernoulliTS(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            alpha=1,\n",
    "            beta=1,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_ts_naive\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        BernoulliTS(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            alpha=1,\n",
    "            beta=100,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_ts_pessimistic\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        ExploreThenCommit(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            min_n=20,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_etc_exploit\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        ExploreThenCommit(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            min_n=100,\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_etc_explore\",\n",
    "    ),\n",
    "    SegmentPolicy(\n",
    "        KLUpperConfidenceBound(\n",
    "            n_actions=deezer_dataset.n_actions,\n",
    "            len_list=deezer_dataset.len_list,\n",
    "            batch_size=1,\n",
    "            random_state=1,\n",
    "            policy_name=\"kl_ucp\",\n",
    "        ),\n",
    "        n_segments=100,\n",
    "        policy_name=\"seg_kl_ucp\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_experiment = DeezerExperiment(\n",
    "    dataset=deezer_dataset,\n",
    "    policies=[\n",
    "        (policy, {\"users_per_batch\": 20000})\n",
    "        if policy.policy_name[:3] != \"lin\"\n",
    "        else (policy, {\"users_per_batch\": 1000})\n",
    "        for policy in policies_cont\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8:01:26 INFO: Running experiment\n",
      "8:01:26 INFO: Learning and obtaining policy feedback\n",
      "8:01:26 INFO: [1 of 10] Learning and obtaining linear_epsilon_greedy_0.01 feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [28:56<00:00, 57.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8:30:23 INFO: [2 of 10] Learning and obtaining linear_epsilon_greedy_0.1 feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [53:04<00:00, 31.40it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:23:28 INFO: [3 of 10] Learning and obtaining kl_ucp feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating online learning: 100%|██████████| 2000000/2000000 [22:28<00:00, 1483.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:47:06 INFO: [4 of 10] Learning and obtaining seg_egreedy_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [06:37<00:00, 5029.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:54:24 INFO: [5 of 10] Learning and obtaining seg_egreedy_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [06:47<00:00, 4912.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:01:55 INFO: [6 of 10] Learning and obtaining seg_ts_naive feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [10:49<00:00, 3078.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:13:28 INFO: [7 of 10] Learning and obtaining seg_ts_pessimistic feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [10:33<00:00, 3156.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:24:44 INFO: [8 of 10] Learning and obtaining seg_etc_exploit feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [09:36<00:00, 3470.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:35:06 INFO: [9 of 10] Learning and obtaining seg_etc_explore feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [10:52<00:00, 3067.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:46:47 INFO: [10 of 10] Learning and obtaining seg_kl_ucp feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 2000000/2000000 [19:18<00:00, 1725.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:06:54 INFO: Done in 9382.91 seconds\n",
      "11:06:54 INFO: Estimating reward confidence interval for random baseline feedback\n",
      "11:06:54 INFO: [1 of 10] Estimating reward confindence interval for linear_epsilon_greedy_0.01 feedback\n",
      "11:06:55 INFO: [2 of 10] Estimating reward confindence interval for linear_epsilon_greedy_0.1 feedback\n",
      "11:06:56 INFO: [3 of 10] Estimating reward confindence interval for kl_ucp feedback\n",
      "11:07:12 INFO: [4 of 10] Estimating reward confindence interval for seg_egreedy_exploit feedback\n",
      "11:07:28 INFO: [5 of 10] Estimating reward confindence interval for seg_egreedy_explore feedback\n",
      "11:07:51 INFO: [6 of 10] Estimating reward confindence interval for seg_ts_naive feedback\n",
      "11:08:11 INFO: [7 of 10] Estimating reward confindence interval for seg_ts_pessimistic feedback\n",
      "11:08:27 INFO: [8 of 10] Estimating reward confindence interval for seg_etc_exploit feedback\n",
      "11:08:43 INFO: [9 of 10] Estimating reward confindence interval for seg_etc_explore feedback\n",
      "11:08:59 INFO: [10 of 10] Estimating reward confindence interval for seg_kl_ucp feedback\n",
      "11:09:15 INFO: Done in 140.57 seconds\n",
      "11:09:15 INFO: Experiment finished in 9523.48 seconds\n"
     ]
    }
   ],
   "source": [
    "deezer_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../results_deezer_8_17.pickle\",\"wb\") as f:\n",
    "    pickle.dump(deezer_experiment.output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy_feedback': {'linear_epsilon_greedy_0.01': {'reward': array([0, 0, 0, ..., 1, 1, 1]),\n",
       "   'n_rounds': 345584,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 65, 65, 65], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'linear_epsilon_greedy_0.1': {'reward': array([0, 0, 0, ..., 1, 1, 1]),\n",
       "   'n_rounds': 359584,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 65, 65, 65], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'kl_ucp': {'reward': array([0, 0, 0, ..., 0, 0, 0]),\n",
       "   'n_rounds': 7560780,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'seg_egreedy_exploit': {'reward': array([0, 0, 0, ..., 0, 0, 0]),\n",
       "   'n_rounds': 7870019,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'seg_egreedy_explore': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 7997768,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'seg_ts_naive': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 7139914,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'seg_ts_pessimistic': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 6896234,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'seg_etc_exploit': {'reward': array([0, 0, 0, ..., 1, 0, 1]),\n",
       "   'n_rounds': 6685155,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'seg_etc_explore': {'reward': array([0, 0, 0, ..., 0, 0, 0]),\n",
       "   'n_rounds': 6559998,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)},\n",
       "  'seg_kl_ucp': {'reward': array([0, 0, 0, ..., 0, 0, 0]),\n",
       "   'n_rounds': 6681131,\n",
       "   'n_actions': 862,\n",
       "   'segments': array([34, 34, 34, ..., 66, 66, 66], dtype=int8),\n",
       "   'batches': array([ 0,  0,  0, ..., 99, 99, 99], dtype=int8)}},\n",
       " 'policies': [(LinEpsilonGreedy(dim=97, n_actions=862, len_list=12, batch_size=349656, alpha_=1.0, lambda_=1.0, random_state=1, epsilon=0.01),\n",
       "   {'users_per_batch': 1000}),\n",
       "  (LinEpsilonGreedy(dim=97, n_actions=862, len_list=12, batch_size=359584, alpha_=1.0, lambda_=1.0, random_state=1, epsilon=0.1),\n",
       "   {'users_per_batch': 1000}),\n",
       "  (KLUpperConfidenceBound(n_actions=862, len_list=12, batch_size=7560780, random_state=1, precision=1e-06, eps=1e-15, policy_name='kl_ucp'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=EpsilonGreedy(n_actions=862, len_list=12, batch_size=1, random_state=1, epsilon=0.01, policy_name='egreedy_1.0'), n_segments=100, policy_name='seg_egreedy_exploit'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=EpsilonGreedy(n_actions=862, len_list=12, batch_size=1, random_state=1, epsilon=0.1, policy_name='egreedy_1.0'), n_segments=100, policy_name='seg_egreedy_explore'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=BernoulliTS(n_actions=862, len_list=12, batch_size=1, random_state=1, alpha=1, beta=1, is_zozotown_prior=False, campaign=None, policy_name='bts'), n_segments=100, policy_name='seg_ts_naive'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=BernoulliTS(n_actions=862, len_list=12, batch_size=1, random_state=1, alpha=1, beta=100, is_zozotown_prior=False, campaign=None, policy_name='bts'), n_segments=100, policy_name='seg_ts_pessimistic'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=ExploreThenCommit(n_actions=862, len_list=12, batch_size=1, random_state=1, min_n=20, policy_name='etc_20'), n_segments=100, policy_name='seg_etc_exploit'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=ExploreThenCommit(n_actions=862, len_list=12, batch_size=1, random_state=1, min_n=100, policy_name='etc_100'), n_segments=100, policy_name='seg_etc_explore'),\n",
       "   {'users_per_batch': 20000}),\n",
       "  (SegmentPolicy(base_policy=KLUpperConfidenceBound(n_actions=862, len_list=12, batch_size=1, random_state=1, precision=1e-06, eps=1e-15, policy_name='kl_ucp'), n_segments=100, policy_name='seg_kl_ucp'),\n",
       "   {'users_per_batch': 20000})],\n",
       " 'reward_summary': {'linear_epsilon_greedy_0.01': {'mean': 0.16083701791749616,\n",
       "   '95.0% CI (lower)': 0.15964085432195935,\n",
       "   '95.0% CI (upper)': 0.16185804319644428},\n",
       "  'linear_epsilon_greedy_0.1': {'mean': 0.22970154400640744,\n",
       "   '95.0% CI (lower)': 0.22834004015751536,\n",
       "   '95.0% CI (upper)': 0.23090376657470854},\n",
       "  'kl_ucp': {'mean': 0.19774721126656247,\n",
       "   '95.0% CI (lower)': 0.1974994246625348,\n",
       "   '95.0% CI (upper)': 0.19801217929896123},\n",
       "  'seg_egreedy_exploit': {'mean': 0.12175934136880735,\n",
       "   '95.0% CI (lower)': 0.12156380753337445,\n",
       "   '95.0% CI (upper)': 0.12195965155865571},\n",
       "  'seg_egreedy_explore': {'mean': 0.09874229910144931,\n",
       "   '95.0% CI (lower)': 0.09853613220588545,\n",
       "   '95.0% CI (upper)': 0.09896130707967524},\n",
       "  'seg_ts_naive': {'mean': 0.23786299106683917,\n",
       "   '95.0% CI (lower)': 0.23758172017197965,\n",
       "   '95.0% CI (upper)': 0.23813638371554618},\n",
       "  'seg_ts_pessimistic': {'mean': 0.2825170419101208,\n",
       "   '95.0% CI (lower)': 0.2822209049170895,\n",
       "   '95.0% CI (upper)': 0.2828817474871067},\n",
       "  'seg_etc_exploit': {'mean': 0.14700132756832116,\n",
       "   '95.0% CI (lower)': 0.1468103619138225,\n",
       "   '95.0% CI (upper)': 0.14722906200379796},\n",
       "  'seg_etc_explore': {'mean': 0.02690644417879395,\n",
       "   '95.0% CI (lower)': 0.02679958515231255,\n",
       "   '95.0% CI (upper)': 0.02703024452141601},\n",
       "  'seg_kl_ucp': {'mean': 0.055210584854570276,\n",
       "   '95.0% CI (lower)': 0.055062859117715246,\n",
       "   '95.0% CI (upper)': 0.05538083297573419}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deezer_experiment.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

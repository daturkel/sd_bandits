{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from obp.dataset.real import OpenBanditDataset\n",
    "from obp.ope import ReplayMethod, InverseProbabilityWeighting, SelfNormalizedInverseProbabilityWeighting, DirectMethod\n",
    "from obp.policy import EpsilonGreedy, BernoulliTS, Random, LinEpsilonGreedy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sd_bandits.experiment import OBDExperiment\n",
    "from sd_bandits.obp_extensions.policy import ExploreThenCommit, KLUpperConfidenceBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    "    datefmt=\"%-I:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad hoc Zozo experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danturkel/.pyenv/versions/3.7.9/envs/sd_bandits/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "obp_dataset = OpenBanditDataset(\n",
    "    behavior_policy=\"random\",\n",
    "    campaign=\"all\",\n",
    "    data_path=Path(\"../data/open_bandit_dataset/\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_basic = [\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        epsilon=0.01,\n",
    "        policy_name=\"egreedy_exploit\",\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        epsilon=0.1,\n",
    "        policy_name=\"egreedy_explore\",\n",
    "    ),\n",
    "    BernoulliTS(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        alpha=1,\n",
    "        beta=1,\n",
    "        policy_name=\"ts_naive\",\n",
    "    ),\n",
    "    BernoulliTS(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        alpha=1,\n",
    "        beta=100,\n",
    "        policy_name=\"ts_pessimistic\",\n",
    "    ),\n",
    "    LinEpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        dim=26,\n",
    "        epsilon=0.01,\n",
    "        #policy_name=\"lin_egreedy_explore\",\n",
    "    ),\n",
    "    LinEpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        dim=26,\n",
    "        epsilon=0.1,\n",
    "        #policy_name=\"lin_egreedy_exploit\",\n",
    "    ),\n",
    "    ExploreThenCommit(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        min_n=20,\n",
    "        policy_name=\"etc_exploit\",\n",
    "    ),\n",
    "    ExploreThenCommit(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        min_n=100,\n",
    "        policy_name=\"etc_explore\",\n",
    "    ),\n",
    "    KLUpperConfidenceBound(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=1,\n",
    "        policy_name=\"kl_ucp\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_experiment_basic = OBDExperiment(\n",
    "    dataset=obp_dataset,\n",
    "    policies=[(policy, {}) for policy in policies_basic],\n",
    "    estimators=[\n",
    "        ReplayMethod(),\n",
    "        InverseProbabilityWeighting(),\n",
    "        SelfNormalizedInverseProbabilityWeighting(),\n",
    "        DirectMethod(),\n",
    "    ],\n",
    "    regression_base_model=LogisticRegression(\n",
    "        max_iter=10000, C=1000, random_state=12345\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:18:04 INFO: Running experiment\n",
      "5:18:04 INFO: Obtaining logged feedback\n",
      "5:18:04 INFO: Done in 0.0 seconds\n",
      "5:18:04 INFO: Fitting regression model\n",
      "5:23:47 INFO: Done in 342.61 seconds\n",
      "5:23:47 INFO: Running simulations\n",
      "5:23:47 INFO: [1 of 9] Running simulation for egreedy_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:19<00:00, 72071.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:24:10 INFO: [2 of 9] Running simulation for egreedy_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:23<00:00, 59587.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:24:38 INFO: [3 of 9] Running simulation for ts_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:59<00:00, 23067.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:25:42 INFO: [4 of 9] Running simulation for ts_pessimistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [01:01<00:00, 22515.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:26:48 INFO: [5 of 9] Running simulation for linear_epsilon_greedy_0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [21:48<00:00, 1050.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:48:41 INFO: [6 of 9] Running simulation for linear_epsilon_greedy_0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [19:32<00:00, 1172.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:08:17 INFO: [7 of 9] Running simulation for etc_exploit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:33<00:00, 41522.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:08:55 INFO: [8 of 9] Running simulation for etc_explore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:33<00:00, 40916.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:09:32 INFO: [9 of 9] Running simulation for kl_ucp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:20<00:00, 65774.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:09:58 INFO: Done in 2770.86 seconds\n",
      "6:09:58 INFO: Estimating rewards\n",
      "6:09:58 INFO: [1 of 4] Estimator ReplayMethod\n",
      "6:09:58 INFO:   [1 of 10] Estimating reward confidence interval for logged\n",
      "6:10:01 INFO:   [2 of 10] Estimating rewards and reward confidence interval for egreedy_exploit\n",
      "6:10:07 INFO:   [3 of 10] Estimating rewards and reward confidence interval for egreedy_explore\n",
      "6:10:13 INFO:   [4 of 10] Estimating rewards and reward confidence interval for ts_naive\n",
      "6:10:18 INFO:   [5 of 10] Estimating rewards and reward confidence interval for ts_pessimistic\n",
      "6:10:23 INFO:   [6 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.01\n",
      "6:10:29 INFO:   [7 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.1\n",
      "6:10:34 INFO:   [8 of 10] Estimating rewards and reward confidence interval for etc_exploit\n",
      "6:10:39 INFO:   [9 of 10] Estimating rewards and reward confidence interval for etc_explore\n",
      "6:10:45 INFO:   [10 of 10] Estimating rewards and reward confidence interval for kl_ucp\n",
      "6:10:50 INFO: [2 of 4] Estimator InverseProbabilityWeighting\n",
      "6:10:50 INFO:   [1 of 10] Estimating reward confidence interval for logged\n",
      "6:10:54 INFO:   [2 of 10] Estimating rewards and reward confidence interval for egreedy_exploit\n",
      "6:10:59 INFO:   [3 of 10] Estimating rewards and reward confidence interval for egreedy_explore\n",
      "6:11:05 INFO:   [4 of 10] Estimating rewards and reward confidence interval for ts_naive\n",
      "6:11:11 INFO:   [5 of 10] Estimating rewards and reward confidence interval for ts_pessimistic\n",
      "6:11:16 INFO:   [6 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.01\n",
      "6:11:22 INFO:   [7 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.1\n",
      "6:11:27 INFO:   [8 of 10] Estimating rewards and reward confidence interval for etc_exploit\n",
      "6:11:32 INFO:   [9 of 10] Estimating rewards and reward confidence interval for etc_explore\n",
      "6:11:38 INFO:   [10 of 10] Estimating rewards and reward confidence interval for kl_ucp\n",
      "6:11:43 INFO: [3 of 4] Estimator SelfNormalizedInverseProbabilityWeighting\n",
      "6:11:43 INFO:   [1 of 10] Estimating reward confidence interval for logged\n",
      "6:11:47 INFO:   [2 of 10] Estimating rewards and reward confidence interval for egreedy_exploit\n",
      "6:11:52 INFO:   [3 of 10] Estimating rewards and reward confidence interval for egreedy_explore\n",
      "6:11:58 INFO:   [4 of 10] Estimating rewards and reward confidence interval for ts_naive\n",
      "6:12:03 INFO:   [5 of 10] Estimating rewards and reward confidence interval for ts_pessimistic\n",
      "6:12:09 INFO:   [6 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.01\n",
      "6:12:14 INFO:   [7 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.1\n",
      "6:12:20 INFO:   [8 of 10] Estimating rewards and reward confidence interval for etc_exploit\n",
      "6:12:26 INFO:   [9 of 10] Estimating rewards and reward confidence interval for etc_explore\n",
      "6:12:31 INFO:   [10 of 10] Estimating rewards and reward confidence interval for kl_ucp\n",
      "6:12:37 INFO: [4 of 4] Estimator DirectMethod\n",
      "6:12:37 INFO:   [1 of 10] Estimating reward confidence interval for logged\n",
      "6:12:40 INFO:   [2 of 10] Estimating rewards and reward confidence interval for egreedy_exploit\n",
      "6:13:03 INFO:   [3 of 10] Estimating rewards and reward confidence interval for egreedy_explore\n",
      "6:13:11 INFO:   [4 of 10] Estimating rewards and reward confidence interval for ts_naive\n",
      "6:13:20 INFO:   [5 of 10] Estimating rewards and reward confidence interval for ts_pessimistic\n",
      "6:13:28 INFO:   [6 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.01\n",
      "6:13:37 INFO:   [7 of 10] Estimating rewards and reward confidence interval for linear_epsilon_greedy_0.1\n",
      "6:13:46 INFO:   [8 of 10] Estimating rewards and reward confidence interval for etc_exploit\n",
      "6:13:54 INFO:   [9 of 10] Estimating rewards and reward confidence interval for etc_explore\n",
      "6:14:03 INFO:   [10 of 10] Estimating rewards and reward confidence interval for kl_ucp\n",
      "6:14:11 INFO: Done in 253.67 seconds\n",
      "6:14:11 INFO: Experiment finished in 3367.15 seconds\n"
     ]
    }
   ],
   "source": [
    "replay_experiment_basic.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results_obp_basic.pickle\", \"wb\") as f:\n",
    "    pickle.dump(replay_experiment_basic.output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy_feedback': {'logged': {'n_rounds': 1374327,\n",
       "   'n_actions': 80,\n",
       "   'reward': array([0, 0, 0, ..., 0, 0, 0])},\n",
       "  'egreedy_exploit': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00603657, 0.00520461, 0.00470689, ..., 0.00232626, 0.00187273,\n",
       "           0.00306799])}},\n",
       "  'egreedy_explore': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00603657, 0.00520461, 0.00470689, ..., 0.00216984, 0.00230421,\n",
       "           0.00281619])}},\n",
       "  'ts_naive': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00407994, 0.00407913, 0.00329216, ..., 0.00212185, 0.00159809,\n",
       "           0.00264788])}},\n",
       "  'ts_pessimistic': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00344779, 0.00343557, 0.00329216, ..., 0.00222036, 0.0016953 ,\n",
       "           0.00265832])}},\n",
       "  'linear_epsilon_greedy_0.01': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00401103, 0.00502384, 0.0037991 , ..., 0.00211063, 0.00169093,\n",
       "           0.00210896])}},\n",
       "  'linear_epsilon_greedy_0.1': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00401103, 0.00502384, 0.0037991 , ..., 0.0027968 , 0.00222601,\n",
       "           0.00249619])}},\n",
       "  'etc_exploit': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00290874, 0.00411427, 0.00452205, ..., 0.00265475, 0.00191511,\n",
       "           0.00292768])}},\n",
       "  'etc_explore': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00290874, 0.00411427, 0.00452205, ..., 0.002256  , 0.00193353,\n",
       "           0.00242185])}},\n",
       "  'kl_ucp': {'reward': {'ReplayMethod': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'InverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'SelfNormalizedInverseProbabilityWeighting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "    'DirectMethod': array([0.00290874, 0.00411427, 0.00452205, ..., 0.00248222, 0.00206119,\n",
       "           0.00306436])}}},\n",
       " 'policies': [(EpsilonGreedy(n_actions=80, len_list=3, batch_size=1, random_state=1, epsilon=0.01, policy_name='egreedy_exploit'),\n",
       "   {}),\n",
       "  (EpsilonGreedy(n_actions=80, len_list=3, batch_size=1, random_state=1, epsilon=0.1, policy_name='egreedy_explore'),\n",
       "   {}),\n",
       "  (BernoulliTS(n_actions=80, len_list=3, batch_size=1, random_state=1, alpha=1, beta=1, is_zozotown_prior=False, campaign=None, policy_name='ts_naive'),\n",
       "   {}),\n",
       "  (BernoulliTS(n_actions=80, len_list=3, batch_size=1, random_state=1, alpha=1, beta=100, is_zozotown_prior=False, campaign=None, policy_name='ts_pessimistic'),\n",
       "   {}),\n",
       "  (LinEpsilonGreedy(dim=26, n_actions=80, len_list=3, batch_size=1, alpha_=1.0, lambda_=1.0, random_state=1, epsilon=0.01),\n",
       "   {}),\n",
       "  (LinEpsilonGreedy(dim=26, n_actions=80, len_list=3, batch_size=1, alpha_=1.0, lambda_=1.0, random_state=1, epsilon=0.1),\n",
       "   {}),\n",
       "  (ExploreThenCommit(n_actions=80, len_list=3, batch_size=1, random_state=1, min_n=20, policy_name='etc_exploit'),\n",
       "   {}),\n",
       "  (ExploreThenCommit(n_actions=80, len_list=3, batch_size=1, random_state=1, min_n=100, policy_name='etc_explore'),\n",
       "   {}),\n",
       "  (KLUpperConfidenceBound(n_actions=80, len_list=3, batch_size=1, random_state=1, precision=1e-06, eps=1e-15, policy_name='kl_ucp'),\n",
       "   {})],\n",
       " 'reward_summary': {'logged': {'mean': 0.0034761887090917964,\n",
       "   '95.0% CI (lower)': 0.0033740150633728362,\n",
       "   '95.0% CI (upper)': 0.003591994481662661},\n",
       "  'egreedy_exploit': {'mean': 0.003803409653163071,\n",
       "   '95.0% CI (lower)': 0.0038015496073757877,\n",
       "   '95.0% CI (upper)': 0.0038050610301199493},\n",
       "  'egreedy_explore': {'mean': 0.0037261801490588936,\n",
       "   '95.0% CI (lower)': 0.003724071640230593,\n",
       "   '95.0% CI (upper)': 0.00372827248908196},\n",
       "  'ts_naive': {'mean': 0.003508374319034018,\n",
       "   '95.0% CI (lower)': 0.00350657258384258,\n",
       "   '95.0% CI (upper)': 0.0035097459658247846},\n",
       "  'ts_pessimistic': {'mean': 0.0034923660276118523,\n",
       "   '95.0% CI (lower)': 0.0034906093580778196,\n",
       "   '95.0% CI (upper)': 0.003494073820713945},\n",
       "  'linear_epsilon_greedy_0.01': {'mean': 0.003448451162469722,\n",
       "   '95.0% CI (lower)': 0.003446862051516129,\n",
       "   '95.0% CI (upper)': 0.0034501047939314066},\n",
       "  'linear_epsilon_greedy_0.1': {'mean': 0.0036487670939721727,\n",
       "   '95.0% CI (lower)': 0.0036467052446097397,\n",
       "   '95.0% CI (upper)': 0.0036507293421626933},\n",
       "  'etc_exploit': {'mean': 0.0037417628702603807,\n",
       "   '95.0% CI (lower)': 0.003739805861827486,\n",
       "   '95.0% CI (upper)': 0.0037436587600213275},\n",
       "  'etc_explore': {'mean': 0.0034972359487707477,\n",
       "   '95.0% CI (lower)': 0.003495404168321325,\n",
       "   '95.0% CI (upper)': 0.003499070828013577},\n",
       "  'kl_ucp': {'mean': 0.00403880128847741,\n",
       "   '95.0% CI (lower)': 0.004037087784589403,\n",
       "   '95.0% CI (upper)': 0.0040405710629135595}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_experiment_basic.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

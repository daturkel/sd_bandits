{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Open Bandit Pipeline\n",
    "\n",
    "![](https://github.com/st-tech/zr-obp/blob/master/images/overview.png?raw=true)\n",
    "\n",
    "**[Official documentation for OBP](https://zr-obp.readthedocs.io/en/latest/)**\n",
    "\n",
    "## Quick Reference:\n",
    "\n",
    "**OBP: Open Bandit Pipeline** - this software library\n",
    "\n",
    "**OBD: Open Bandit Dataset** - the dataset supplied with it\n",
    "\n",
    "**OPE: off-policy evaluation** - the process of determining how a policy _other than the one that was really run_ woudl have performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader\n",
    "\n",
    "The first part of the Open Bandit Pipeline (OBP) is the dataset loader. For the Open Bandit Dataset (OBD), the loader is `opb.dataset.OpenBanditDataset` ([docs](https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.dataset.real.html#obp.dataset.real.OpenBanditDataset)). \n",
    "\n",
    "As with many classes in the OBP, the dataset modules are implemented with [dataclasses](https://docs.python.org/3.7/library/dataclasses.html).\n",
    "\n",
    "The dataset module inherits from `obp.dataset.base.BaseRealBanditDatset` ([docs](https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.dataset.base.html#module-obp.dataset.base)) and should implement three methods:\n",
    "- `load_raw_data()`: Load an on-disk representation of the dataset into the module. Used during initialization.\n",
    "- `pre_process()`: Perform any preprocessing needed to transform the raw data representation into a final representation.\n",
    "- `obtain_batch_bandit_feedback()`: Return a dictionary containing (at least) keys: `[\"action\",\"position\",\"reward\",\"pscore\",\"context\",\"n_rounds\"]`\n",
    "\n",
    "It is also helpful if the dataset module exposes a property `len_list`, which is how many items the bandit shows the user at a time. Often the answer is 1, though in the case of OBD it's 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danturkel/.pyenv/versions/3.7.9/envs/sd_bandits/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from obp.dataset import OpenBanditDataset\n",
    "\n",
    "DATASET = \"./data/obd_full\"\n",
    "# DATASET = \"./data/obd\"\n",
    "\n",
    "dataset = OpenBanditDataset(\n",
    "    data_path=Path(DATASET),\n",
    "    campaign=\"all\",\n",
    "    behavior_policy=\"random\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method OpenBanditDataset.load_raw_data of OpenBanditDataset(behavior_policy='random', campaign='all', data_path=PosixPath('data/obd_full/random/all'), dataset_name='obd')>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the OBD load_raw_data doesn't need to be manually called but it's used internally\n",
    "\n",
    "dataset.load_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method OpenBanditDataset.pre_process of OpenBanditDataset(behavior_policy='random', campaign='all', data_path=PosixPath('data/obd_full/random/all'), dataset_name='obd')>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see the minimal preprocessing done by OBD, see source:\n",
    "# https://zr-obp.readthedocs.io/en/latest/_modules/obp/dataset/real.html#OpenBanditDataset.pre_process\n",
    "\n",
    "dataset.pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback dict:\n",
      "  n_rounds: <class 'int'>\n",
      "  n_actions: <class 'int'>\n",
      "  action: <class 'numpy.ndarray'>\n",
      "  position: <class 'numpy.ndarray'>\n",
      "  reward: <class 'numpy.ndarray'>\n",
      "  reward_test: <class 'numpy.ndarray'>\n",
      "  pscore: <class 'numpy.ndarray'>\n",
      "  context: <class 'numpy.ndarray'>\n",
      "  action_context: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "feedback = dataset.obtain_batch_bandit_feedback()\n",
    "print(\"feedback dict:\")\n",
    "for key, value in feedback.items():\n",
    "    print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.len_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and Simulator\n",
    "\n",
    "The policy object defines a bandit policy counterfactual—i.e. a bandit strategy that is not the same as what you ran in actuality. \n",
    "\n",
    "Policies are typicially initialized with `n_actions` (how many arms), `len_list` (how many arms can be shown at a time), `batch_size` (how often do we update parameters?)\n",
    "\n",
    "The task of a policy object can be seen through what methods it implements:\n",
    "- `initialize()`: set policy parameter starting values\n",
    "- `select_action()`: decide which action to take (i.e. which arm to pull) (can be multiple actions if len_list > 1)\n",
    "- `update_params(action, reward)`: update the policy parameters based on the action chosen and the reward received\n",
    "\n",
    "Policies are initialized with a few parameters, depending on whether they are contextual or context-free. Information on those parameters can be found [here](https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.policy.base.html).\n",
    "\n",
    "**Note:** While many of Zozo's tutorials show the `BernoulliTS` policy being used and calling its `compute_batch_action_dist` function, this is actually not how most policies are run and it only works because `BernoulliTS` (and `Random`) is a fully randomized policy. \n",
    "\n",
    "Most policies are instead run using the `obp.sumulator.simulator.run_bandit_simulation(bandit_feedback, policy)` method ([docs](https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.simulator.simulator.html#module-obp.simulator.simulator))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opportunity: Better Batching\n",
    "\n",
    "The policies can update their parameters every `batch_size` rounds, but if we have access to timestamps, it might be more valuable to update parameters every _day_, to better simulate how actual training would likely look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obp.policy import EpsilonGreedy\n",
    "\n",
    "eps_greedy = EpsilonGreedy(\n",
    "    n_actions=dataset.n_actions,\n",
    "    len_list = dataset.len_list,\n",
    "    batch_size=1, # update parameters after every round\n",
    "    random_state=0,\n",
    "    epsilon=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:34<00:00, 39334.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of returned actions array: (1374327, 80, 3)\n",
      "\n",
      "# data points: 1374327\n",
      "# actions ('arms'): 80\n",
      "# positions: 3\n"
     ]
    }
   ],
   "source": [
    "from obp.simulator.simulator import run_bandit_simulation\n",
    "\n",
    "actions = run_bandit_simulation(bandit_feedback=feedback, policy=eps_greedy)\n",
    "\n",
    "print(f\"\\nShape of returned actions array: {actions.shape}\\n\")\n",
    "print(f\"# data points: {dataset.n_rounds}\")\n",
    "print(f\"# actions ('arms'): {dataset.n_actions}\")\n",
    "print(f\"# positions: {dataset.len_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy Evaluation\n",
    "\n",
    "The off-policy evaluator (or OPE) is responsible for determining how our chosen policy would have performed. Running the simulator on the policy gives us the actions it would have taken (or some probabilities), but now we need to know what kind of rewards those actions would lead to.\n",
    "\n",
    "This is easier said than done. One way to do it is to go through the log data and only use the rewards of cases when the logged action matches the action your new policy would've taken. This could easily be a bad estimate if the choices of the original policy look nothing like the choices the new policy would make (though if your original policy was uniform random, that's helpful). \n",
    "\n",
    "Methods like inverse propensity weighting try to compensate for the likelihood of an action being in the logged data, though it does nothing in the event of uniform random data since all actions were uniformly random.\n",
    "\n",
    "Other methods try to learn a regression model so that they can predict rewards for any user-action pair, though these methods are heavily reliant on that model being accurate.\n",
    "\n",
    "An off-policy estimator implements two methods:\n",
    "- `estimate_policy_value()` which gets an average reward per round\n",
    "- `estimate_interval()` which gets a confidence interval of rewards via bootstrap\n",
    "\n",
    "OPE methods which rely on regression to model rewards need to be given those rewards at initialization time. OBP provides a wrapper `obp.ope.regression_model` ([docs](https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.ope.regression_model.html)) for using an off-the-shelf regression model for that piece of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opportunity: Better evaluation than mean reward\n",
    "\n",
    "Maybe we don't think mean reward is a particularly interesting metric. A method which does heavy exploration at the beginning will likely get low rewards at the beginning, but might end up getting much better. It could be worth comparing graphs of rolling average rewards to see how well and how fast different bandit policies learn over the course of the logged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'reward_test', 'pscore', 'context', 'action_context'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obp.ope.estimators import ReplayMethod\n",
    "\n",
    "replay_evaluator = ReplayMethod()\n",
    "\n",
    "results = replay_evaluator.estimate_interval(\n",
    "    reward=feedback[\"reward\"],\n",
    "    action=feedback[\"action\"],\n",
    "    position=feedback[\"position\"],\n",
    "    action_dist=actions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=0.0047\n",
      "lower=0.0037\n",
      "upper=0.0057\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean={np.round(results['mean'],4)}\")\n",
    "print(f\"lower={np.round(results['95.0% CI (lower)'],4)}\")\n",
    "print(f\"upper={np.round(results['95.0% CI (upper)'],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth value = 0.0035\n"
     ]
    }
   ],
   "source": [
    "ground_truth_mean = feedback[\"reward\"].mean()\n",
    "print(f\"ground truth value = {np.round(ground_truth_mean,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative improvement: 1.3422\n"
     ]
    }
   ],
   "source": [
    "print(f\"relative improvement: {np.round(results['mean'] / ground_truth_mean,4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

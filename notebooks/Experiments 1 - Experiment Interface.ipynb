{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from obp.dataset.real import OpenBanditDataset\n",
    "from obp.ope import DirectMethod, ReplayMethod\n",
    "from obp.policy import EpsilonGreedy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sd_bandits.obp_extensions.dataset import DeezerDataset\n",
    "from sd_bandits.experiment import DeezerExperiment, OBDExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    "    datefmt=\"%-I:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment interface\n",
    "\n",
    "In this notebook, we show off the `Experiment` interface, which is implemented as `OBDExperiment` and `DeezerExperiment`. It provides uniform instantiation and a `run_experiment` method to make it easier for our script to run lots of experiments.\n",
    "\n",
    "## 1. An experiment on OBD dataset with two epsilon greedy policies and direct method\n",
    "\n",
    "OBD experiments are done using Zozo's logged data. Policies are learned offline using the `run_bandit_simulation` method from OBP, then are evaluated using off-policy estimators. In this experiment, we use an off-policy estimator which requires a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danturkel/.pyenv/versions/3.7.9/envs/sd_bandits/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "obp_dataset = OpenBanditDataset(\n",
    "    behavior_policy=\"random\",\n",
    "    campaign=\"all\",\n",
    "    data_path=Path(\"../data/open_bandit_dataset/\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.05,\n",
    "        policy_name=\"egreedy_0.05\"\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.7,\n",
    "        policy_name=\"egreedy_0.7\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obd_experiment = OBDExperiment(\n",
    "    dataset=obp_dataset,\n",
    "    policies=[(policy,{}) for policy in policies],\n",
    "    estimator=DirectMethod(),\n",
    "    regression_base_model=LinearRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:30:30 INFO: Running experiment\n",
      "6:30:30 INFO: Obtaining logged feedback\n",
      "6:30:30 INFO: Done in 0.0 seconds\n",
      "6:30:30 INFO: Fitting regression model\n",
      "6:31:38 INFO: Done in 67.46 seconds\n",
      "6:31:38 INFO: Running simulations\n",
      "6:31:38 INFO: [1 of 2] Running simulation for egreedy_0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:20<00:00, 68294.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:32:01 INFO: [2 of 2] Running simulation for egreedy_0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:30<00:00, 45296.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:32:36 INFO: Done in 57.96 seconds\n",
      "6:32:36 INFO: Estimating rewards\n",
      "6:32:36 INFO: [1 of 3] Estimating reward confidence interval for logged\n",
      "6:32:39 INFO: [2 of 3] Estimating rewards and reward confidence interval for egreedy_0.05\n",
      "6:32:46 INFO: [3 of 3] Estimating rewards and reward confidence interval for egreedy_0.7\n",
      "6:32:52 INFO: Done in 16.29 seconds\n",
      "6:32:52 INFO: Experiment finished in 141.72 seconds\n"
     ]
    }
   ],
   "source": [
    "obd_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logged': {'mean': 0.0034761887090917964,\n",
       "  '95.0% CI (lower)': 0.0033740150633728362,\n",
       "  '95.0% CI (upper)': 0.003591994481662661},\n",
       " 'egreedy_0.05': {'mean': 0.0037086448762773437,\n",
       "  '95.0% CI (lower)': 0.0037064967699232875,\n",
       "  '95.0% CI (upper)': 0.0037106488262537212},\n",
       " 'egreedy_0.7': {'mean': 0.0034654071772142175,\n",
       "  '95.0% CI (lower)': 0.003463383846547121,\n",
       "  '95.0% CI (upper)': 0.0034670289864836834}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obd_experiment.reward_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback for logged:\n",
      "  n_rounds\n",
      "  n_actions\n",
      "  action\n",
      "  position\n",
      "  reward\n",
      "  reward_test\n",
      "  pscore\n",
      "  context\n",
      "  action_context\n",
      "Feedback for egreedy_0.05:\n",
      "  action\n",
      "  reward\n",
      "Feedback for egreedy_0.7:\n",
      "  action\n",
      "  reward\n"
     ]
    }
   ],
   "source": [
    "for policy_name, feedback in obd_experiment.policy_feedback.items():\n",
    "    print(f\"Feedback for {policy_name}:\")\n",
    "    print(\"\\n\".join([f\"  {key}\" for key in feedback.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An experiment on OBP dataset with two epsilon greedy policies and replay method (no regression)\n",
    "\n",
    "Similar to experiment 1, but much faster as we use a (less accurate) estimator which does not require a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.05,\n",
    "        policy_name=\"egreedy_0.05\"\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.7,\n",
    "        policy_name=\"egreedy_0.7\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obd_experiment_replay = OBDExperiment(\n",
    "    dataset=obp_dataset,\n",
    "    policies=[(policy,{}) for policy in policies],\n",
    "    estimator=ReplayMethod(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:32:52 INFO: Running experiment\n",
      "6:32:52 INFO: Obtaining logged feedback\n",
      "6:32:52 INFO: Done in 0.0 seconds\n",
      "6:32:52 INFO: Running simulations\n",
      "6:32:52 INFO: [1 of 2] Running simulation for egreedy_0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:19<00:00, 70059.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:33:15 INFO: [2 of 2] Running simulation for egreedy_0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:30<00:00, 44713.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:33:50 INFO: Done in 58.27 seconds\n",
      "6:33:50 INFO: Estimating rewards\n",
      "6:33:50 INFO: [1 of 3] Estimating reward confidence interval for logged\n",
      "6:33:54 INFO: [2 of 3] Estimating rewards and reward confidence interval for egreedy_0.05\n",
      "6:33:57 INFO: [3 of 3] Estimating rewards and reward confidence interval for egreedy_0.7\n",
      "6:34:00 INFO: Done in 9.94 seconds\n",
      "6:34:00 INFO: Experiment finished in 68.22 seconds\n"
     ]
    }
   ],
   "source": [
    "obd_experiment_replay.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logged': {'mean': 0.0034761887090917964,\n",
       "  '95.0% CI (lower)': 0.0033740150633728362,\n",
       "  '95.0% CI (upper)': 0.003591994481662661},\n",
       " 'egreedy_0.05': {'mean': 0.0027236942600317245,\n",
       "  '95.0% CI (lower)': 0.0020254391633864038,\n",
       "  '95.0% CI (upper)': 0.003497150578696902},\n",
       " 'egreedy_0.7': {'mean': 0.0037752285135881263,\n",
       "  '95.0% CI (lower)': 0.0028832586957855365,\n",
       "  '95.0% CI (upper)': 0.004600944727317347}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obd_experiment_replay.reward_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An experiment on Deezer dataset\n",
    "\n",
    "Deezer experiments are run by generating random feedback as a baseline, then simulating online policy learning for the supplied policies. \n",
    "\n",
    "No estimators are needed since the learning is always done online. Instead we just obtain bootstrap estimates of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_dataset = DeezerDataset(\"../data/deezer_carousel_bandits/user_features.csv\",\"../data/deezer_carousel_bandits/playlist_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.05,\n",
    "        policy_name=\"egreedy_0.05\"\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.7,\n",
    "        policy_name=\"egreedy_0.7\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_experiment = DeezerExperiment(\n",
    "    dataset=deezer_dataset,\n",
    "    policies=[(policy, {\"users_per_batch\": 1000}) for policy in policies],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:34:20 INFO: Running experiment\n",
      "6:34:20 INFO: Learning and obtaining policy feedback\n",
      "6:34:20 INFO: [1 of 2] Learning and obtaining egreedy_0.05 feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:18<00:00, 5436.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:34:40 INFO: [2 of 2] Learning and obtaining egreedy_0.7 feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5160.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6:35:01 INFO: Done in 41.24 seconds\n",
      "6:35:01 INFO: Estimating reward confidence interval for random baseline feedback\n",
      "6:35:01 INFO: [1 of 2] Estimating reward confindence interval for egreedy_0.05 feedback\n",
      "6:35:02 INFO: [2 of 2] Estimating reward confindence interval for egreedy_0.7 feedback\n",
      "6:35:02 INFO: Done in 1.35 seconds\n",
      "6:35:02 INFO: Experiment finished in 42.6 seconds\n"
     ]
    }
   ],
   "source": [
    "deezer_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'egreedy_0.05': {'mean': 0.08347886720843115,\n",
       "  '95.0% CI (lower)': 0.08258542383748002,\n",
       "  '95.0% CI (upper)': 0.08438901745330148},\n",
       " 'egreedy_0.7': {'mean': 0.05765443921227229,\n",
       "  '95.0% CI (lower)': 0.056970242570535394,\n",
       "  '95.0% CI (upper)': 0.058470447020959465}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deezer_experiment.reward_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

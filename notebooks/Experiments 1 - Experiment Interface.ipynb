{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from obp.dataset.real import OpenBanditDataset\n",
    "from obp.ope import DirectMethod, ReplayMethod\n",
    "from obp.policy import EpsilonGreedy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sd_bandits.obp_extensions.dataset import DeezerDataset\n",
    "from sd_bandits.experiment import DeezerExperiment, OBDExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout,\n",
    "    datefmt=\"%-I:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment interface\n",
    "\n",
    "In this notebook, we show off the `Experiment` interface, which is implemented as `OBDExperiment` and `DeezerExperiment`. It provides uniform instantiation and a `run_experiment` method to make it easier for our script to run lots of experiments.\n",
    "\n",
    "## 1. An experiment on OBD dataset with two epsilon greedy policies and direct method\n",
    "\n",
    "OBD experiments are done using Zozo's logged data. Policies are learned offline using the `run_bandit_simulation` method from OBP, then are evaluated using off-policy estimators. In this experiment, we use an off-policy estimator which requires a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danturkel/.pyenv/versions/3.7.9/envs/sd_bandits/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "obp_dataset = OpenBanditDataset(\n",
    "    behavior_policy=\"random\",\n",
    "    campaign=\"all\",\n",
    "    data_path=Path(\"../data/open_bandit_dataset/\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.05,\n",
    "        policy_name=\"egreedy_0.05\"\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.7,\n",
    "        policy_name=\"egreedy_0.7\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obd_experiment = OBDExperiment(\n",
    "    dataset=obp_dataset,\n",
    "    policies=policies,\n",
    "    estimator=DirectMethod(),\n",
    "    regression_base_model=LinearRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:01 INFO: Obtaining logged feedback\n",
      "12:11:01 INFO: Done in 0.0 seconds\n",
      "12:11:01 INFO: Fitting regression model\n",
      "12:12:18 INFO: Done in 76.91 seconds\n",
      "12:12:18 INFO: Running simulations\n",
      "12:12:18 INFO: [1 of 2] Running simulation for egreedy_0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:20<00:00, 66949.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:12:42 INFO: [2 of 2] Running simulation for egreedy_0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:32<00:00, 42649.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:19 INFO: Done in 61.22 seconds\n",
      "12:13:19 INFO: Estimating rewards\n",
      "12:13:19 INFO: Estimating reward confidence interval for logged feedback\n",
      "12:13:23 INFO: [1 of 2] Estimating rewards for egreedy_0.05\n",
      "12:19:06 INFO: [2 of 2] Estimating rewards for egreedy_0.7\n",
      "12:24:37 INFO: Done in 677.96 seconds\n",
      "12:24:37 INFO: Experiment finished in 816.1 seconds\n"
     ]
    }
   ],
   "source": [
    "obd_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logged': {'mean': 0.0034739403358880383,\n",
       "  '95.0% CI (lower)': 0.0033819462180398113,\n",
       "  '95.0% CI (upper)': 0.0035743312908790995},\n",
       " 'egreedy_0.05': {'mean': 0.0037086705687790038,\n",
       "  '95.0% CI (lower)': 0.003706822027564767,\n",
       "  '95.0% CI (upper)': 0.003710537497724463},\n",
       " 'egreedy_0.7': {'mean': 0.00346541602435126,\n",
       "  '95.0% CI (lower)': 0.003463509143184019,\n",
       "  '95.0% CI (upper)': 0.0034672870551908943}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obd_experiment.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An experiment on OBP dataset with two epsilon greedy policies and replay method (no regression)\n",
    "\n",
    "Similar to experiment 1, but much faster as we use a (less accurate) estimator which does not require a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.05,\n",
    "        policy_name=\"egreedy_0.05\"\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=obp_dataset.n_actions,\n",
    "        len_list=obp_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.7,\n",
    "        policy_name=\"egreedy_0.7\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "obd_experiment_replay = OBDExperiment(\n",
    "    dataset=obp_dataset,\n",
    "    policies=policies,\n",
    "    estimator=ReplayMethod(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:42 INFO: Obtaining logged feedback\n",
      "12:25:42 INFO: Done in 0.0 seconds\n",
      "12:25:42 INFO: Running simulations\n",
      "12:25:42 INFO: [1 of 2] Running simulation for egreedy_0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:19<00:00, 71630.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:26:06 INFO: [2 of 2] Running simulation for egreedy_0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374327/1374327 [00:34<00:00, 39646.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:26:44 INFO: Done in 62.21 seconds\n",
      "12:26:44 INFO: Estimating rewards\n",
      "12:26:44 INFO: Estimating reward confidence interval for logged feedback\n",
      "12:26:48 INFO: [1 of 2] Estimating rewards for egreedy_0.05\n",
      "12:26:52 INFO: [2 of 2] Estimating rewards for egreedy_0.7\n",
      "12:26:55 INFO: Done in 11.06 seconds\n",
      "12:26:55 INFO: Experiment finished in 73.28 seconds\n"
     ]
    }
   ],
   "source": [
    "obd_experiment_replay.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logged': {'mean': 0.0034739403358880383,\n",
       "  '95.0% CI (lower)': 0.0033819462180398113,\n",
       "  '95.0% CI (upper)': 0.0035743312908790995},\n",
       " 'egreedy_0.05': {'mean': 0.0026614182480465287,\n",
       "  '95.0% CI (lower)': 0.0018462487515422113,\n",
       "  '95.0% CI (upper)': 0.003466306327477821},\n",
       " 'egreedy_0.7': {'mean': 0.0037580516532728075,\n",
       "  '95.0% CI (lower)': 0.0028510520826943147,\n",
       "  '95.0% CI (upper)': 0.004694497270106132}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obd_experiment_replay.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An experiment on Deezer dataset\n",
    "\n",
    "Deezer experiments are run by generating random feedback as a baseline, then simulating online policy learning for the supplied policies. \n",
    "\n",
    "No estimators are needed since the learning is always done online. Instead we just obtain bootstrap estimates of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_dataset = DeezerDataset(\"../data/deezer_carousel_bandits/user_features.csv\",\"../data/deezer_carousel_bandits/playlist_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.05,\n",
    "        policy_name=\"egreedy_0.05\"\n",
    "    ),\n",
    "    EpsilonGreedy(\n",
    "        n_actions=deezer_dataset.n_actions,\n",
    "        len_list=deezer_dataset.len_list,\n",
    "        batch_size=1,\n",
    "        random_state=0,\n",
    "        epsilon=0.7,\n",
    "        policy_name=\"egreedy_0.7\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezer_experiment = DeezerExperiment(\n",
    "    dataset=deezer_dataset,\n",
    "    policies=policies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:40:13 INFO: Obtaining random baseline feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating click probabilities: 100%|██████████| 100000/100000 [00:16<00:00, 5913.63it/s]\n",
      "Generating feedback: 100%|██████████| 100000/100000 [00:04<00:00, 23213.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:40:36 INFO: Done in 22.74 seconds\n",
      "12:40:36 INFO: Learning and obtaining policy feedback\n",
      "12:40:36 INFO: [1 of 2] Learning and obtaining egreedy_0.05 feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:18<00:00, 5440.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:40:57 INFO: [2 of 2] Learning and obtaining egreedy_0.7 feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating online learning: 100%|██████████| 100000/100000 [00:19<00:00, 5223.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:41:17 INFO: Done in 41.19 seconds\n",
      "12:41:17 INFO: Estimating reward confidence interval for random baseline feedback\n",
      "12:41:18 INFO: [1 of 2] Estimating reward confindence interval for egreedy_0.05 feedback\n",
      "12:41:19 INFO: [2 of 2] Estimating reward confindence interval for egreedy_0.7 feedback\n",
      "12:41:20 INFO: Done in 2.19 seconds\n",
      "12:41:20 INFO: Experiment finished in 66.13 seconds\n"
     ]
    }
   ],
   "source": [
    "deezer_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random': {'mean': 0.02699252613151486,\n",
       "  '95.0% CI (lower)': 0.026517594669501272,\n",
       "  '95.0% CI (upper)': 0.027457833749215527},\n",
       " 'egreedy_0.05': {'mean': 0.08347886720843115,\n",
       "  '95.0% CI (lower)': 0.08258542383748002,\n",
       "  '95.0% CI (upper)': 0.08438901745330148},\n",
       " 'egreedy_0.7': {'mean': 0.05765443921227229,\n",
       "  '95.0% CI (lower)': 0.056970242570535394,\n",
       "  '95.0% CI (upper)': 0.058470447020959465}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deezer_experiment.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
